{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c65975d",
   "metadata": {},
   "source": [
    "# YaRN (Yet another RoPE extensioN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63679cab",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728a450",
   "metadata": {},
   "source": [
    "[YaRN(Yet another RoPE extensioN method)](https://arxiv.org/abs/2309.00071) is a compute-efficient way to extend the context window of RoPE-based models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e624daf6",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d2421",
   "metadata": {},
   "source": [
    "RoPE injects position information through rotation, which lets the model learn position without addtional parameters while preserving vector norms. However, RoPE has a fundamental limitation: models can only handle sequences no longer than they have seen during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa93a9",
   "metadata": {},
   "source": [
    "RoPE encodes position through frequency($\\phi_m(p)$)-dependent rotations, where:\n",
    "$$\n",
    "\\phi_m(p) = p \\cdot \\theta_m \\quad \\text{where} \\quad \\theta_m = B^{\\frac{-2m}{d_{head}}}\n",
    "$$ \n",
    "When position $p$ exceeds the longest sequence in training, model encounters position patterns that it hasn't learned and this could hurt inference performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1434a80",
   "metadata": {},
   "source": [
    "RoPE's frequency design is inherently multi-scale. Across the whole embedding dimensions:\n",
    "- **High frequencie**s (low dimension $m$): capture fine-grained positional relationships, allowing the model to learn *local attention patterns*\n",
    "- **Low frequencies** (high dimension $m$): capture coarse-grained positional relationships, allowing the model to learn *global attention patterns*\n",
    "\n",
    "For each token, RoPE encodes its position information with rotation frequencies of all embedding dimensions. This naturally becomes a multi-scale position signature and the model learns to use different attention heads to focus on different frequecny ranges - some become specialized in local relationships, others in global relationships. On the flip side, this makes naive position scaling to account for unseen context length very difficult for RoPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d50fc8",
   "metadata": {},
   "source": [
    "### Previous Scaling Attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd1003",
   "metadata": {},
   "source": [
    "#### Linear Interpolation (Position Interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb01817",
   "metadata": {},
   "source": [
    "Introduced by Chen et al. from Meta in their 2023 paper [Extending Context Window of Large Language Models via Position Interpolation](https://arxiv.org/abs/2306.15595), Linear Interpolation compresses position space uniformly to account for the target context length:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccedebf",
   "metadata": {},
   "source": [
    "$$\n",
    "s = \\frac{L'}{L} \\quad , \\quad m' = \\frac{m}{s}  \\quad \\text{(m is position index)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a397d6",
   "metadata": {},
   "source": [
    "This paper uses a different notation from the RoPE paper which can be confusing, in RoPE paper, $m$ stands for dimension index while $p$ for token position. To keep it consistent, let's keep $p$ for position index and rewrute the full Linear Interpolation RoPE formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b63d8",
   "metadata": {},
   "source": [
    "$$\n",
    "s = \\frac{L'}{L} \\quad , \\quad p' = \\frac{p}{s}\n",
    "$$\n",
    "$$\n",
    "\\phi'(p,m) = p' \\cdot B^{\\frac{-2m}{d_{head}}} = \\frac{p}{s} \\cdot B^{\\frac{-2m}{d_{head}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c4e6f6",
   "metadata": {},
   "source": [
    "In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9cc179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def linear_interpolation_rope(max_seq_len, head_dim, original_max_len, base=10000.0):\n",
    "    scale = max_seq_len / original_max_len\n",
    "    positions = torch.arange(max_seq_len) / scale # Compress positions\n",
    "    # The rest of RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369d66c",
   "metadata": {},
   "source": [
    "The problem with this approach is that uniform compression hurts all frequencies equally, especially over-compressing high-frequency information hurts performance of fine-grained local attentions.\n",
    "\n",
    "RoPE doesn't use a single notion of \"position\". It encodes each token position with rotation frequencies across the whole embedding dimensions. Therefore, every token position is represented through many rotary frequencies at once - a multi-scale position signature with the higher frequencies (lower dimensions) representing fine-grained, local structure and the lower frequencies (higher dimensions) for coarse, global structure. The model sees position at multiple scales simultaneously and learns to use different attention heads to focus on different frequency ranges, some heads become specialized in local relationships and others in global relationships.\n",
    "\n",
    "This makes it very difficult to use just the naive scaling methods like Positional Interpolation (PI) and keep the model performing well. PI uniformly scales the token positions (p to p/s, s=L'/L), reducing the relative rotation angle differences across the entire frequency spectrum (embedding dimensions): In high-frequency (low dimension) regions, the sharp rotary angle contrast become dampened, making it difficult for the model to distinguish and interpret local relationships; in low-frequency (high dimension) regions, where the rotary angles already change very slowly, the scaled angle differences become negligibly subtle, making the model lose all its discriminative power for long-range relationships.\n",
    "\n",
    "As a result, the model's entire multi-scale balance collapses and its learned attention patterns can no longer be applied to the new sequence lengths. This is why we need other context scaling methods working with RoPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e41dc7",
   "metadata": {},
   "source": [
    "#### NTK-aware Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b5961a",
   "metadata": {},
   "source": [
    "NTK(Neural Tangent Kernel)-aware scaling emerged from the Reddit community r/LocalLLaMA in mid-2023. It was developed through community experimentations and implemented in popular inference tools like [llama.cpp](https://github.com/ggml-org/llama.cpp), [text-generation-webui](https://github.com/oobabooga/text-generation-webui), and various Hugging Face model implementations.\n",
    "\n",
    "We've just discussed how naive position interpolation directly scales the token position and stretches the position frequency space uniformly, skewing the multi-scale position signature and hurting the model performance. NTK-aware scaling, on the other hand, doesn't scale the position directly, but applies a dimension-wise adjustment to each rotary frequency based on the target and original context length ratio. As a result, it reshapes the rotary frequency spectrum, scaling lower dimensions (higher frequencies) less and higher dimensions (lower frequencies) more, so that the model's learned multi-scale positional signals stay in a usable regime and the model can still perform optimally with context windows much longer than it has seen during training.\n",
    "\n",
    "The formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3dd028",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_m = B^{\\frac{-2m}{d_{head}}} \\quad \\text{(RoPE)}\n",
    "$$\n",
    "$$\n",
    "\\theta_m^{\\text{NTK}} = \\theta_m \\cdot s^{\\frac{2m}{d-2}} \\quad \\text{where} \\quad s = \\frac{L'}{L}\n",
    "$$\n",
    "$$\n",
    "\\phi_m^{NTK}(p) = {p} \\cdot \\theta_m^{\\text{NTK}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed45432",
   "metadata": {},
   "source": [
    "This formula was derived empirically rather than from formal mathematical derivation. Its heuristics:\n",
    "- Low dimensions (high frequency): $m$ is small so frequencies get minimal scaling, keeping the fine-grained local attention performance\n",
    "- High dimensions (low frequency): $m$ is large so frequencies get agressive scaling. This works since the coarse-grained global positional relationships (represented in high dimensions) are more robust to compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05cf12bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk_scaling_rope(max_seq_len, head_dim, original_max_len, base=10000.0):\n",
    "    scale = max_seq_len / original_max_len\n",
    "    m = torch.arrange(head_dim // 2)\n",
    "    # original theta_m\n",
    "    theta_m = base ** (-2 * m / head_dim)\n",
    "    # NTK-aware scaling\n",
    "    theta_m_ntk = theta_m * (scale ** (2 * m / (head_dim - 2)))\n",
    "    # ... Rest of RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95378943",
   "metadata": {},
   "source": [
    "### Dynamic NTK Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b09bfef",
   "metadata": {},
   "source": [
    "At inference-time, often mutiple forward-passes are performed with varying sequence lengths from 1 to max context length (e.g., autoregressive token generation). Throughout this inference cycle, we can either \n",
    "- apply the same positional embedding scaling (PI or NTK-aware Scaling), or\n",
    "- update position embedding scaling for every sequence length from 1 to max_seq_length\n",
    "\n",
    "The first method causes the model to perform sub-optimal for sequnce length smaller than the max_seq_length, also an abrupt performance degradation when the sequence length is longer than the max_seq_length. The second method allows the model to gracefully degrade. When combined with NTK-aware scaling, the second method is called **Dynamic NTK Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32f7ceb",
   "metadata": {},
   "source": [
    "### YaRN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a44ef5",
   "metadata": {},
   "source": [
    "YaRN was the first formal academic paper that rigorously analyzed and improved upon community experimentations to address the fundamental issue of extending context window of RoPE models: different frequency bands require different scaling strategies. It was introduced by Peng et al. in their 2023 paper [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cfa72b",
   "metadata": {},
   "source": [
    "Standard RoPE fails when extending beyond the max context window that the model has seen in training. To address this, Linear Interpolation applies a uniform position compression. NTK-aware scaling employs dimension-dependent scaling. YaRN improves LI and NTK-aware scaling by:\n",
    "- **NTK-by-parts interpolations** with a ramp function: NTK-by-parts partitions RoPE frequencies into different regions with differnet scaling strategies, while the ramp function smoothly transitions between these interpolation and extrapolation regions to avoid discontinuities.\n",
    "  - High frequencies (local patterns): interpolation (gentle compression)\n",
    "  - Low frequencies (global patterns): extrapolation (preserve as-is)\n",
    "- **Attention scaling temperature**: a scaling temperature factor `t` is added to control how aggressively the rotary frequencies diverge across dimensions. This prevents the attention distribution from collapsing into overly sharp or overly flat patterns as the context window grows, keeping the attention entropy in the regime the model was originally trained on.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb86845",
   "metadata": {},
   "source": [
    "With this design YaRN achieves strong long-context performance with minimal fine-tuning (only ~400 steps of continued pretraining/fine-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561f90b",
   "metadata": {},
   "source": [
    "The core YaRN formula:\n",
    "\n",
    "**NTK-by-parts Interpolation with Ramp Function**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b299e",
   "metadata": {},
   "source": [
    "We choose not to interpolate the high frequency dimensions at all while always interpolating the lower frequency dimensions. In particular:\n",
    "- For small wavelengths $\\lambda$ (much smaller than $L$, aka high frequencies/low dimension m), extrapolate, don't interpolate (don't compress $\\theta_m$)\n",
    "- For large wavelengths $\\lambda$ (equal to or bigger than $L$, aka low frequencies/high dimension m), interpolate (compress) and avoid extrapolation(avoid preserving)\n",
    "- For dimensions in-between, we can have a bit of both (weighted sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1cca5",
   "metadata": {},
   "source": [
    "So the NTK-by-parts interpolation formula is a weighted sum of interpolation (scaling) and extrapolation (not scaling). The weight is dependent on dimension $m$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d24bc2",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_m' = (1 - \\gamma (r(m)))\\frac{\\theta_m}{s} + \\gamma(r(m))\\theta_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b79235",
   "metadata": {},
   "source": [
    "Where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15649c43",
   "metadata": {},
   "source": [
    "- $s$ is the context length scale factor: $s = \\frac{L'}{L}$\n",
    "- $r(m)$ is the ratio between the original context size $L$ and the wavelength at dimension $m$, $\\lambda_m$:\n",
    "$r(m) = \\frac{L}{\\lambda_m}$ \n",
    "- The wavelength $\\lambda_m$ is calculated from that dimension's frequency $\\theta_m$:\n",
    "$\\lambda_m = \\frac{2\\pi}{\\theta_m} = \\frac{2\\pi}{B^{\\frac{-2m}{d_{head}}}} = 2\\pi B^{\\frac{2m}{d_{head}}}$\n",
    "- $\\gamma(r)$ is the **Ramp function**:\n",
    "$$\n",
    "\\gamma(r) = \n",
    "\\begin{cases} \n",
    "0, & \\text{if } r < \\alpha \\\\ \n",
    "1, & \\text{if } r > \\beta \\\\   \n",
    "\\frac{r - \\alpha}{\\beta - \\alpha}, & \\text{otherwise }\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ and $\\beta$ are two tunable parameters that should be tuned on a case-by-case basis. E.g., the authors found that for Llama family models, good values are $\\alpha=1$ and $\\beta=32$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96066422",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "367e6303",
   "metadata": {},
   "source": [
    "**Attention Scaling Temperature**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb84a12",
   "metadata": {},
   "source": [
    "When extending the context length far beyond what the model was trained on, the model has far more tokens to attend to and the attention is spread thinner. The attention score distribution thus becomes unstable and exhibits high entropy (blurred or flat), breaking the trained attention regime. This makes it harder to distinguish important information and noise and the model starts to lose focus. \n",
    "\n",
    "To address this, YaRN introduced an attention scaling temperature t to dampen or shrink the overall magnitude of the attention scores, preventing them from becoming too large and unstable. The t is applied on the logits before attention softmax as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74af137",
   "metadata": {},
   "source": [
    "$$\n",
    "softmax(\\frac{q_m @ k_n^T}{t\\sqrt{d_k}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b5335",
   "metadata": {},
   "source": [
    "This is to address the issue that attention score distribution changes when extending context length. When context window is extended, the attention mechanism sees more tokens and the distribution of attention scores are impacted:\n",
    "- More tokens to attend to -> attention is spread thinner\n",
    "- Average attention entropy increases\n",
    "This can lead to problems such as diluted attention patterns, loss of focus on important tokens, and thus degrade the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522b03a",
   "metadata": {},
   "source": [
    "For LLaMA and Llama 2 models, the authors recommended:\n",
    "$$\n",
    "\\sqrt{\\frac{1}{t}} = 0.1 ln(s) + 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b4ad6",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7f639",
   "metadata": {},
   "source": [
    "Let's implement YaRN following the formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc257e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def build_yarn_rope_cache(\n",
    "    dim: int,\n",
    "    max_seq_len: int, # L'\n",
    "    orig_seq_len:int,   # L\n",
    "    base: float = 10000.0,\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 32.0,\n",
    "    device = None,\n",
    "    dtype = torch.float32\n",
    "):\n",
    "    ################\n",
    "    # NTK-by-parts #\n",
    "    ################\n",
    "\n",
    "    # Scale factor s = L'/L\n",
    "    s = max_seq_len / orig_seq_len\n",
    "\n",
    "    # Original RoPE frequencies\n",
    "    # theta_m = B^(-2m/d_head)\n",
    "    m = torch.arange(dim // 2, device=device, dtype=dtype)\n",
    "    theta_m = base ** (-2.0 * m / dim)\n",
    "\n",
    "    # Wavelengths\n",
    "    # lambda_m = 2pi / theta_m\n",
    "    lambda_m = 2 * math.pi / theta_m\n",
    "\n",
    "    # ratio r(m) = L / lambda_m\n",
    "    r_m = orig_seq_len / lambda_m\n",
    "\n",
    "    # ramp function gamma(r)\n",
    "    gamma = torch.zeros_like(r_m)\n",
    "    # if r < alpha: gamma = 0\n",
    "    # if r > beta: gamma = 1\n",
    "    # else: gamma = (r - alpha) / (beta - alpha)\n",
    "    mask_middle = (r_m >= alpha) & (r_m <= beta)\n",
    "    mask_high = r_m > beta\n",
    "    gamma[mask_middle] = (r_m[mask_middle] - alpha) / (beta - alpha)\n",
    "    gamma[mask_high] = 1.0\n",
    "\n",
    "    # NTK-by-parts interpolation: theta_m' = (1 - gamma(r(m)) * theta_m / s + gamma(r(m)) * theta_m\n",
    "    theta_yarn = (1 - gamma) * (theta_m / s) + gamma * theta_m\n",
    "\n",
    "    #################################\n",
    "    # Attention scaling temperature #\n",
    "    #################################\n",
    "\n",
    "    # mscale = sqrt(1/t) = 0.1 * ln(s) + 1 => \n",
    "    mscale = 0.1 * math.log(s) + 1.0 if s > 1 else 1.0\n",
    "\n",
    "    return theta_yarn, mscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae31b2",
   "metadata": {},
   "source": [
    "An example to apply YaRN in a RoPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ee7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_yarn_rope(\n",
    "    q: torch.Tensor,\n",
    "    k: torch.Tensor,\n",
    "    positions: torch.Tensor,\n",
    "    max_seq_len: int,\n",
    "    orig_seq_len: int = 2048,\n",
    "    base: float = 10000.0,\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 32.0\n",
    "):\n",
    "    head_dim = q.size(-1)\n",
    "    device = q.device\n",
    "\n",
    "    # Get NTK-by-parts theta_m_yarn, attention scaling temperature mscale\n",
    "    theta_m_yarn, mscale = build_yarn_rope_cache(\n",
    "        head_dim, max_seq_len, orig_seq_len, base, alpha, beta, device, q.dtype\n",
    "    )\n",
    "\n",
    "    # RoPE\n",
    "    # Compute rotation angles: phi_m(p) = p * theta_m'\n",
    "    # shape: (positions, head_dim // 2)\n",
    "    angles = positions[:, None] * theta_m_yarn[None, :]\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    ## Split q, k into even/odd pairs\n",
    "    q_even = q[..., 0::2]\n",
    "    q_odd = q[..., 1::2]\n",
    "    k_even = k[..., 0::2]\n",
    "    k_odd = k[..., 1::2]\n",
    "    ## Rotation matrix\n",
    "    q_rotated_even = cos * q_even - sin * q_odd\n",
    "    q_rotated_odd = sin * q_even + cos * q_odd\n",
    "    k_rotated_even = cos * k_even - sin * k_odd\n",
    "    k_rotated_odd = sin * k_even + cos * k_odd\n",
    "    ## Interleave back\n",
    "    q_rotated = torch.empty_like(q)\n",
    "    q_rotated[..., 0::2] = q_rotated_even\n",
    "    q_rotated[..., 1::2] = q_rotated_odd\n",
    "    k_rotated = torch.empty_like(k)\n",
    "    k_rotated[..., 0::2] = k_rotated_even\n",
    "    k_rotated[..., 1::2] = k_rotated_odd\n",
    "\n",
    "    # Apply temperature mscale\n",
    "    q_rotated = q_rotated * mscale\n",
    "    k_rotated = k_rotated * mscale\n",
    "\n",
    "    return q_rotated, k_rotated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89aff32",
   "metadata": {},
   "source": [
    "An example to use YaRN RoPE in MHA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25da1a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class YaRNRopeMHA(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, orig_max_length = 2048, \n",
    "                 dropout=0.1, alpha=1.0, beta=32.0, base=10000.0):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.orig_max_length = orig_max_length\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.base = base\n",
    "\n",
    "        # Initialize Q, K, V, output projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        # Initialize dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def _apply_yarn_rope(self, x, positions, max_seq_len):\n",
    "        device = x.device\n",
    "\n",
    "        theta_m_yarn, mscale = build_yarn_rope_cache(\n",
    "            self.head_dim, \n",
    "            max_seq_len, self.orig_max_length, \n",
    "            self.base, self.alpha, self.beta, \n",
    "            device, x.dtype\n",
    "        )\n",
    "        \n",
    "        # positions: (batch_size, seq_len) -> angles: (batch_size, seq_len, head_dim//2)\n",
    "        angles = positions.unsqueeze(-1).float() * theta_m_yarn.unsqueeze(0).unsqueeze(0)\n",
    "        cos = torch.cos(angles)\n",
    "        sin = torch.sin(angles)\n",
    "\n",
    "        # Add num_heads dimension: (batch_size, seq_len, head_dim//2) -> (batch_size, seq_len, 1, head_dim//2)\n",
    "        cos = cos.unsqueeze(2)\n",
    "        sin = sin.unsqueeze(2)\n",
    "        \n",
    "        x_even = x[..., 0::2]\n",
    "        x_odd = x[..., 1::2]\n",
    "        x_rotated_even = cos * x_even - sin * x_odd\n",
    "        x_rotated_odd = sin * x_even + cos * x_odd\n",
    "        x_rotated = torch.empty_like(x)\n",
    "        x_rotated[..., 0::2] = x_rotated_even\n",
    "        x_rotated[..., 1::2] = x_rotated_odd\n",
    "        x_rotated = x_rotated * mscale\n",
    "\n",
    "        return x_rotated\n",
    "    \n",
    "    def forward(self, x, positions, max_seq_len=None, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        if max_seq_len is None:\n",
    "            max_seq_len = self.orig_max_length\n",
    "        \n",
    "        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Apply YaRN RoPE to Q, K\n",
    "        Q = self._apply_yarn_rope(Q, positions, max_seq_len)\n",
    "        K = self._apply_yarn_rope(K, positions, max_seq_len)\n",
    "\n",
    "        # Parallel multi-head attention\n",
    "        Q = Q.transpose(1,2)\n",
    "        K = K.transpose(1,2)\n",
    "        V = V.transpose(1,2)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        attn_values = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Reshape\n",
    "        output = attn_values.transpose(1,2).reshape(batch_size, seq_len, d_model)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a6540",
   "metadata": {},
   "source": [
    "Test extending context length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08c234a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 8192, 512])\n",
      "Output shape: torch.Size([2, 8192, 512])\n",
      "Context extension: 4.0x\n"
     ]
    }
   ],
   "source": [
    "# model config\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "orig_seq_len = 2048\n",
    "\n",
    "mha = YaRNRopeMHA(d_model, num_heads, orig_seq_len)\n",
    "\n",
    "batch_size = 2\n",
    "extended_seq_len = 8192 # 4x\n",
    "\n",
    "# Input\n",
    "x = torch.randn(batch_size, extended_seq_len, d_model)\n",
    "positions = torch.arange(extended_seq_len).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "# Forward pass with extended context\n",
    "output = mha(x, positions, max_seq_len=extended_seq_len)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Context extension: {extended_seq_len / orig_seq_len}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0052e2",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb462668",
   "metadata": {},
   "source": [
    "- EleutherAI blog: [Extending the RoPE](https://blog.eleuther.ai/yarn/) 2023/11/13"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
