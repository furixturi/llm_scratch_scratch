{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c65975d",
   "metadata": {},
   "source": [
    "# YaRN (Yet another RoPE extensioN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63679cab",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728a450",
   "metadata": {},
   "source": [
    "[YaRN(Yet another RoPE extensioN method)](https://arxiv.org/abs/2309.00071) is a compute-efficient way to extend the context window of RoPE-based models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e624daf6",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d2421",
   "metadata": {},
   "source": [
    "RoPE injects position information through rotation, which lets the model learn position without addtional parameters while preserving vector norms. However, RoPE has a fundamental limitation: models can only handle sequences no longer than they have seen during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa93a9",
   "metadata": {},
   "source": [
    "RoPE encodes position through frequency($\\phi_m(p)$)-dependent rotations, where:\n",
    "$$\n",
    "\\phi_m(p) = p \\cdot \\theta_m \\quad \\text{where} \\quad \\theta_m = B^{\\frac{-2m}{d_{head}}}\n",
    "$$ \n",
    "When position $p$ exceeds the longest sequence in training, model encounters position patterns that it hasn't learned and this could hurt inference performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1434a80",
   "metadata": {},
   "source": [
    "RoPE's frequency design is inherently multi-scale. Across the whole embedding dimensions:\n",
    "- **High frequencie**s (low dimension $m$): capture fine-grained positional relationships, allowing the model to learn *local attention patterns*\n",
    "- **Low frequencies** (high dimension $m$): capture coarse-grained positional relationships, allowing the model to learn *global attention patterns*\n",
    "\n",
    "For each token, RoPE encodes its position information with rotation frequencies of all embedding dimensions. This naturally becomes a multi-scale position signature and the model learns to use different attention heads to focus on different frequecny ranges - some become specialized in local relationships, others in global relationships. On the flip side, this makes naive position scaling to account for unseen context length very difficult for RoPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d50fc8",
   "metadata": {},
   "source": [
    "### Previous Scaling Attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd1003",
   "metadata": {},
   "source": [
    "#### Linear Interpolation (Position Interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb01817",
   "metadata": {},
   "source": [
    "Introduced by Chen et al. from Meta in their 2023 paper [Extending Context Window of Large Language Models via Position Interpolation](https://arxiv.org/abs/2306.15595), Linear Interpolation compresses position space uniformly to account for the target context length:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccedebf",
   "metadata": {},
   "source": [
    "$$\n",
    "s = \\frac{L'}{L} \\quad , \\quad m' = \\frac{m}{s}  \\quad \\text{(m is position index)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a397d6",
   "metadata": {},
   "source": [
    "This paper uses a different notation from the RoPE paper which can be confusing, in RoPE paper, $m$ stands for dimension index while $p$ for token position. To keep it consistent, let's keep $p$ for position index and rewrute the full Linear Interpolation RoPE formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b63d8",
   "metadata": {},
   "source": [
    "$$\n",
    "s = \\frac{L'}{L} \\quad , \\quad p' = \\frac{p}{s}\n",
    "$$\n",
    "$$\n",
    "\\phi'(p,m) = p' \\cdot B^{\\frac{-2m}{d_{head}}} = \\frac{p}{s} \\cdot B^{\\frac{-2m}{d_{head}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c4e6f6",
   "metadata": {},
   "source": [
    "In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9cc179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def linear_interpolation_rope(max_seq_len, head_dim, original_max_len, base=10000.0):\n",
    "    scale = max_seq_len / original_max_len\n",
    "    positions = torch.arange(max_seq_len) / scale # Compress positions\n",
    "    # The rest of RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369d66c",
   "metadata": {},
   "source": [
    "The problem with this approach is that uniform compression hurts all frequencies equally, especially over-compressing high-frequency information hurts performance of fine-grained local attentions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e41dc7",
   "metadata": {},
   "source": [
    "#### NTK-aware Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b5961a",
   "metadata": {},
   "source": [
    "NTK(Neural Tangent Kernel)-aware scaling emerged from the Reddit community r/LocalLLaMA in mid-2023. It was developed through community experientations and implemented in popular inference tools like [llama.cpp](https://github.com/ggml-org/llama.cpp), [text-generation-webui](https://github.com/oobabooga/text-generation-webui), and various Hugging Face model implementations.\n",
    "\n",
    "Instead of scaling positions, this approach scales $\\theta_m$ depending on dimension $m$, slowing down the angular growth at higher frequencies (lower dimensions).\n",
    "\n",
    "The formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3dd028",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_m = B^{\\frac{-2m}{d_{head}}} \\quad \\text{(RoPE)}\n",
    "$$\n",
    "$$\n",
    "\\theta_m^{\\text{NTK}} = \\theta_m \\cdot s^{\\frac{2m}{d-2}} \\quad \\text{where} \\quad s = \\frac{L'}{L}\n",
    "$$\n",
    "$$\n",
    "\\phi_m^{NTK}(p) = {p} \\cdot \\theta_m^{\\text{NTK}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed45432",
   "metadata": {},
   "source": [
    "This formula ws derived empirically rather than from formal mathematical derivation. Its heuristics:\n",
    "- Low dimensions (high frequency): $m$ is small so frequencies get minimal scaling, keeping the fine-grained local attention performance\n",
    "- High dimensions (low frequency): $m$ is large so frequencies get agressive scaling. This works since the coarse-grained global positional relationships (represented in high dimensions) are more robust to compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05cf12bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk_scaling_rope(max_seq_len, head_dim, original_max_len, base=10000.0):\n",
    "    scale = max_seq_len / original_max_len\n",
    "    m = torch.arrange(head_dim // 2)\n",
    "    # original theta_m\n",
    "    theta_m = base ** (-2 * m / head_dim)\n",
    "    # NTK-aware scaling\n",
    "    theta_m_ntk = theta_m * (scale ** (2 * m / (head_dim - 2)))\n",
    "    # ... Rest of RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95378943",
   "metadata": {},
   "source": [
    "### Dynamic NTK Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b09bfef",
   "metadata": {},
   "source": [
    "At inference-time, often mutiple forward-passes are performed with varing sequence lengths from 1 to max context length (e.g., autoregressive token generation). Throughout this inference cycle, we can either \n",
    "- apply the same positional embedding scaling (PI or NTK-aware Scaling), or\n",
    "- update position embedding scaling for every sequence length from 1 to max_seq_length\n",
    "\n",
    "The first method causes the model to perform sub-optimal for sequnce length smaller than the max_seq_length, also an abrupt performance degradation when the sequence length is longer than the max_seq_length. The second method allows the model to gracefully degrade. When combined with NTK-aware scaling, the second method is called **Dynamic NTK Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32f7ceb",
   "metadata": {},
   "source": [
    "### YaRN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a44ef5",
   "metadata": {},
   "source": [
    "YaRN was the first formal academic paper that rigorously analyzed and improved upon community experimentations to address the fundamental issue of extending context window of RoPE models: different frequency bands require different scaling strategies. It was introduced by Peng et al. in their 2023 paper [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cfa72b",
   "metadata": {},
   "source": [
    "Standard RoPE fails when extending beyond the max context window that the model has seen in training. To address this, Linear Interpolation applies a uniform position compression. NTK-aware scaling employs dimension-dependent scaling. YaRN improves LI and NTK-aware scaling by:\n",
    "- **NTK-by-parts interpolations** with a ramp function: NTK-by-parts partitions RoPE frequencies into different regions with differnet scaling strategies, while the ramp function smoothly transitions between these interpolation and extrapolation regions to avoid discontinuities.\n",
    "  - High frequencies (local patterns): interpolation (gentle compression)\n",
    "  - Low frequencies (global patterns): extrapolation (preserve as-is)\n",
    "- **Attention scaling temperature**: add a temperature factor `t` to maintain proper attention entropy\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb86845",
   "metadata": {},
   "source": [
    "With this design YaRN achieves strong long-context performance with minimal fine-tuning (only ~400 steps of continued pretraining/fine-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561f90b",
   "metadata": {},
   "source": [
    "The core YaRN formula:\n",
    "\n",
    "**NTK-by-parts Interpolation with Ramp Function**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b299e",
   "metadata": {},
   "source": [
    "We choose not to interpolate the high frequency dimensions at all while always interpolating the lower frequency dimensions. In particular:\n",
    "- For small wavelengths $\\lambda$ (much smaller than $L$, aka high frequencies/low dimension m), don't interpolate (don't compress $\\theta_m$)\n",
    "- For large wavelengths $\\lambda$ (equal to or bigger than $L$, aka low frequencies/high dimension m), interpolate (compress) and avoid extrapolation(avoid preserving)\n",
    "- For dimensions in-between, we can have a bit of both (weighted sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1cca5",
   "metadata": {},
   "source": [
    "So the NTK-by-parts interpolation formula is a weighted sum of interpolation (scaling) and extrapolation (not scaling). The weight is dependent on dimension $m$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d24bc2",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_m' = (1 - \\gamma (r(m)))\\frac{\\theta_m}{s} + \\gamma(r(m))\\theta_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b79235",
   "metadata": {},
   "source": [
    "Where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15649c43",
   "metadata": {},
   "source": [
    "- $s$ is the context length scale factor\n",
    "- $r(m)$ is the ratio between the original context size $L$ and the wavelength at dimension $m$, $\\lambda_m$:\n",
    "$$\n",
    "s = \\frac{L'}{L}, \\quad r(m) = \\frac{L}{\\lambda_m}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96066422",
   "metadata": {},
   "source": [
    "- The wavelength $\\lambda_m$ is calculated from that dimension's frequency $\\theta_m$:\n",
    "$$\n",
    "\\lambda_m = \\frac{2\\pi}{\\theta_m} = \\frac{2\\pi}{B^{\\frac{-2m}{d_{head}}}} = 2\\pi B^{\\frac{2m}{d_{head}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eac96c",
   "metadata": {},
   "source": [
    "\n",
    "- $\\gamma(r)$ is the **Ramp function**:\n",
    "$$\n",
    "\\gamma(r) = \n",
    "\\begin{cases} \n",
    "0, & \\text{if } r < \\alpha \\\\ \n",
    "1, & \\text{if } r > \\beta \\\\   \n",
    "\\frac{r - \\alpha}{\\beta - \\alpha}, & \\text{otherwise }\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ and $\\beta$ are two tunable parameters that should be tuned on a case-by-case basis. E.g., the authors found that for Llama family models, good values are $\\alpha=1$ and $\\beta=32$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e6303",
   "metadata": {},
   "source": [
    "**Attention Scaling Temperature**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb84a12",
   "metadata": {},
   "source": [
    "YaRN also introduces a temprature $t$ on the logits before attention softmax. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74af137",
   "metadata": {},
   "source": [
    "$$\n",
    "softmax(\\frac{q_m @ k_n^T}{t\\sqrt{d_k}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b5335",
   "metadata": {},
   "source": [
    "This is to address the issue that attention score distribution changes when extending context length. When context window is extended, the attention mechanism sees more tokens and the distribution of attention scores are impacted:\n",
    "- More tokens to attend to -> attention is spread thinner\n",
    "- Average attention entropy increases\n",
    "This can lead to problems such as diluted attention patterns, loss of focus on important tokens, and thus degrade the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522b03a",
   "metadata": {},
   "source": [
    "For LLaMA and Llana 2 models, the authors recommended:\n",
    "$$\n",
    "\\sqrt{\\frac{1}{t}} = 0.1 ln(s) + 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b4ad6",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7f639",
   "metadata": {},
   "source": [
    "Let's implement YaRN following the formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fc257e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def build_yarn_rope_cache(\n",
    "    dim: int,\n",
    "    max_seq_len: int, # L'\n",
    "    orig_seq_len:int,   # L\n",
    "    base: float = 10000.0,\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 32.0,\n",
    "    device = None,\n",
    "    dtype = torch.float32\n",
    "):\n",
    "    # Scale factor s = L'/L\n",
    "    s = max_seq_len / orig_seq_len\n",
    "\n",
    "    # Original RoPE frequencies\n",
    "    # theta_m = B^(-2m/d_head)\n",
    "    m = torch.arange(dim // 2, device=device, dtype=dtype)\n",
    "    theta_m = base ** (-2.0 * m / dim)\n",
    "\n",
    "    # Wavelengths\n",
    "    # lambda_m = 2pi / theta_m\n",
    "    lambda_m = 2 * math.pi / theta_m\n",
    "\n",
    "    # ratio r(m) = L / lambda_m\n",
    "    r_m = orig_seq_len / lambda_m\n",
    "\n",
    "    # ramp function gamma(r)\n",
    "    gamma = torch.zeros_like(r_m)\n",
    "    # if r < alpha: gamma = 0\n",
    "    # if r > beta: gamma = 1\n",
    "    # else: gamma = (r - alpha) / (beta - alpha)\n",
    "    mask_middle = (r_m >= alpha) & (r_m <= beta)\n",
    "    mask_high = r_m > beta\n",
    "    gamma[mask_middle] = (r_m[mask_middle] - alpha) / (beta - alpha)\n",
    "    gamma[mask_high] = 1.0\n",
    "\n",
    "    # NTK-by-parts interpolation\n",
    "    # theta_m' = (1 - gamma(r(m)) * theta_m / s + gamma(r(m)) * theta_m\n",
    "    theta_yarn = (1 - gamma) * (theta_m / s) + gamma * theta_m\n",
    "\n",
    "    # Attention scaling temperature\n",
    "    # mscale = sqrt(1/t) = 0.1 * ln(s) + 1 => \n",
    "    mscale = 0.1 * math.log(s) + 1.0 if s > 1 else 1.0\n",
    "\n",
    "    return theta_yarn, mscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae31b2",
   "metadata": {},
   "source": [
    "An example to apply YaRN in a RoPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b08ee7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_yarn_rope(\n",
    "    q: torch.Tensor,\n",
    "    k: torch.Tensor,\n",
    "    positions: torch.Tensor,\n",
    "    max_seq_len: int,\n",
    "    orig_seq_len: int = 2048,\n",
    "    base: float = 10000.0,\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 32.0\n",
    "):\n",
    "    head_dim = q.size(-1)\n",
    "    device = q.device\n",
    "\n",
    "    # Get theta_yarn, mscale\n",
    "    theta_m_yarn, mscale = build_yarn_rope_cache(\n",
    "        head_dim, max_seq_len, orig_seq_len, base, alpha, beta, device, q.dtype\n",
    "    )\n",
    "\n",
    "    # RoPE\n",
    "    # Compute rotation angles: phi_m(p) = p * theta_m'\n",
    "    # shape: (positions, head_dim // 2)\n",
    "    angles = positions[:, None] * theta_m_yarn[None, :]\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    ## Split q, k into even/odd pairs\n",
    "    q_even = q[..., 0::2]\n",
    "    q_odd = q[..., 1::2]\n",
    "    k_even = k[..., 0::2]\n",
    "    k_odd = k[..., 1::2]\n",
    "    ## Rotation matrix\n",
    "    q_rotated_even = cos * q_even - sin * q_odd\n",
    "    q_rotated_odd = sin * q_even + cos * q_odd\n",
    "    k_rotated_even = cos * k_even - sin * k_odd\n",
    "    k_rotated_odd = sin * k_even + cos * k_odd\n",
    "    ## Interleave back\n",
    "    q_rotated = torch.empty_like(q)\n",
    "    q_rotated[..., 0::2] = q_rotated_even\n",
    "    q_rotated[..., 1::2] = q_rotated_odd\n",
    "    k_rotated = torch.empty_like(k)\n",
    "    k_rotated[..., 0::2] = k_rotated_even\n",
    "    k_rotated[..., 1::2] = k_rotated_odd\n",
    "\n",
    "    # Apply temperature mscale\n",
    "    q_rotated = q_rotated * mscale\n",
    "    k_rotated = k_rotated * mscale\n",
    "\n",
    "    return q_rotated, k_rotated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89aff32",
   "metadata": {},
   "source": [
    "An example to use YaRN RoPE in MHA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25da1a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class YaRNRopeMHA(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, orig_max_length = 2048, \n",
    "                 dropout=0.1, alpha=1.0, beta=32.0, base=10000.0):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.orig_max_length = orig_max_length\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.base = base\n",
    "\n",
    "        # Initialize Q, K, V, output projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        # Initialize dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def _apply_yarn_rope(self, x, positions, max_seq_len):\n",
    "        device = x.device\n",
    "\n",
    "        theta_m_yarn, mscale = build_yarn_rope_cache(\n",
    "            self.head_dim, \n",
    "            max_seq_len, self.orig_max_length, \n",
    "            self.base, self.alpha, self.beta, \n",
    "            device, x.dtype\n",
    "        )\n",
    "        \n",
    "        # positions: (batch_size, seq_len) -> angles: (batch_size, seq_len, head_dim//2)\n",
    "        angles = positions.unsqueeze(-1).float() * theta_m_yarn.unsqueeze(0).unsqueeze(0)\n",
    "        cos = torch.cos(angles)\n",
    "        sin = torch.sin(angles)\n",
    "\n",
    "        # Add num_heads dimension: (batch_size, seq_len, head_dim//2) -> (batch_size, seq_len, 1, head_dim//2)\n",
    "        cos = cos.unsqueeze(2)\n",
    "        sin = sin.unsqueeze(2)\n",
    "        \n",
    "        x_even = x[..., 0::2]\n",
    "        x_odd = x[..., 1::2]\n",
    "        x_rotated_even = cos * x_even - sin * x_odd\n",
    "        x_rotated_odd = sin * x_even + cos * x_odd\n",
    "        x_rotated = torch.empty_like(x)\n",
    "        x_rotated[..., 0::2] = x_rotated_even\n",
    "        x_rotated[..., 1::2] = x_rotated_odd\n",
    "        x_rotated = x_rotated * mscale\n",
    "\n",
    "        return x_rotated\n",
    "    \n",
    "    def forward(self, x, positions, max_seq_len=None, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        if max_seq_len is None:\n",
    "            max_seq_len = self.orig_max_length\n",
    "        \n",
    "        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Apply YaRN RoPE to Q, K\n",
    "        Q = self._apply_yarn_rope(Q, positions, max_seq_len)\n",
    "        K = self._apply_yarn_rope(K, positions, max_seq_len)\n",
    "\n",
    "        # Parallel multi-head attention\n",
    "        Q = Q.transpose(1,2)\n",
    "        K = K.transpose(1,2)\n",
    "        V = V.transpose(1,2)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        attn_values = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Reshape\n",
    "        output = attn_values.transpose(1,2).reshape(batch_size, seq_len, d_model)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a6540",
   "metadata": {},
   "source": [
    "Test extending context length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08c234a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 8192, 512])\n",
      "Output shape: torch.Size([2, 8192, 512])\n",
      "Context extension: 4.0x\n"
     ]
    }
   ],
   "source": [
    "# model config\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "orig_seq_len = 2048\n",
    "\n",
    "mha = YaRNRopeMHA(d_model, num_heads, orig_seq_len)\n",
    "\n",
    "batch_size = 2\n",
    "extended_seq_len = 8192 # 4x\n",
    "\n",
    "# Input\n",
    "x = torch.randn(batch_size, extended_seq_len, d_model)\n",
    "positions = torch.arange(extended_seq_len).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "# Forward pass with extended context\n",
    "output = mha(x, positions, max_seq_len=extended_seq_len)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Context extension: {extended_seq_len / orig_seq_len}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0052e2",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb462668",
   "metadata": {},
   "source": [
    "- EleutherAI blog: [Extending the RoPE](https://blog.eleuther.ai/yarn/) 2023/11/13"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
