{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddaa7da7",
   "metadata": {},
   "source": [
    "# LLM Context Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215762ea",
   "metadata": {},
   "source": [
    "## A Quick Journey to Context Extension Landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9fe10e",
   "metadata": {},
   "source": [
    "Why\n",
    "- tasks like multi-document reasoning, handling long code bases "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049d1fd",
   "metadata": {},
   "source": [
    "Landscape\n",
    "- Early Transformers: 512~1024 tokens (BERT, GPT-2)\n",
    "- Each generation pushed further: GPT-3 (2K-4K), open source long-context models (8K-32K), and now commercial LLMs offering 128K~1M+ tokens (GPT-4, Claude 3.5/4, Gemini 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86391c",
   "metadata": {},
   "source": [
    "## Encoding-Based Context Extension Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d88cee5",
   "metadata": {},
   "source": [
    "### RoPE Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e41fe7",
   "metadata": {},
   "source": [
    "- Position Interpolation\n",
    "- NTK-Aware Scaling\n",
    "- xPos\n",
    "- YaRN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b26ed5",
   "metadata": {},
   "source": [
    "### Non-RoPE Alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c87f80a",
   "metadata": {},
   "source": [
    "- ALiBi\n",
    "- Relative Position Representations (Shaw et al., Transformer-XL, T5)\n",
    "- Recurrence & Memory (Transformer-XL, Compressive Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feed8ac",
   "metadata": {},
   "source": [
    "## Beyond Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed39195",
   "metadata": {},
   "source": [
    "- Sparse Attention (LongFormer, BigBird)\n",
    "- Sliding Window Attention (MPT, GPT-OSS)\n",
    "- Recurrence & Memory (Transformer-XL, Compressive Transformer, RWKV)\n",
    "- Hybrid Layer"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
