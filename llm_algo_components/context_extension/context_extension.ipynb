{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddaa7da7",
   "metadata": {},
   "source": [
    "# LLM Context Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67096fe",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218acef",
   "metadata": {},
   "source": [
    "LLM context length, or context window, is the maximum input length (number of tokens) that the model can process each time. It is the model's \"attention span,\" within which the model can pay attention to for generating the next token. In attention implementation, it is the parameter that usually named `seq_len` (or `max_seq_len`).\n",
    "\n",
    "**[TODO] add why long context is needed (long context tasks, agents)**\n",
    "\n",
    "LLMs are stateless. It is the context window that decides how much information we can give the model each time to get an output, which then decides what tasks the model can do for us. For example, is it able to process a couple of sentences/paragraphs, or a whole bunch of documents, or even entire books and code bases?\n",
    "\n",
    "The attention mechanism has quadratic complexity with regard to the context length (O(N^2)). Extending context length N means quickly increasing computation and memory costs. Therefore, efficient context extension is essential for both evaluation quality and system performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215762ea",
   "metadata": {},
   "source": [
    "## Growing Context Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0c72c",
   "metadata": {},
   "source": [
    "Eraly Transformers had limited context lengths. BERT(2018) and T5(2020) both have a max context length of 512. GPT-2(2019) doubled BERT and made it 1024. GPT-3(2020) doubled GPT-2 to 2048, GPT-3.5(2022) doubled it again to 4096, then GPT-4(2023) did the double game again and pushed it to 8192.\n",
    "\n",
    "With the context window growth from 512 to 8K tokens, the complexity and diversity of tasks that an LLM can handle evolved from sentence-level tasks such as sentiment classification, NER, translation to multi-turn complex dialogues and multi-page document processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049d1fd",
   "metadata": {},
   "source": [
    "Landscape\n",
    "- Early Transformers: 512~1024 tokens (BERT, GPT-2)\n",
    "- Each generation pushed further: GPT-3 (2K-4K), open source long-context models (8K-32K), and now commercial LLMs offering 128K~1M+ tokens (GPT-4, Claude 3.5/4, Gemini 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902a339",
   "metadata": {},
   "source": [
    "### Long-Context Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13de76a3",
   "metadata": {},
   "source": [
    "- Meta: [Effective Long-Context Scaling of Foundation Models](https://arxiv.org/abs/2309.16039), 2023/09\n",
    "- Qwen: [Qwen2.5-1M Technical Report](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf?spm=a2ty_o06.30285417.0.0.2850c921AhQWSG&file=Qwen2_5_1M_Technical_Report.pdf)\n",
    "  - progressive length extension strategy \n",
    "    - agent-generated large-scale instruction data\n",
    "    - multi-stage SFT and RL to ensure balanced performance across short and long sequences, optimizing alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86391c",
   "metadata": {},
   "source": [
    "### Length Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f880eb",
   "metadata": {},
   "source": [
    "#### Non-RoPE\n",
    "- ALiBi\n",
    "- Relative Position Representations (Shaw et al., Transformer-XL, T5)\n",
    "- Recurrence & Memory (Transformer-XL, Compressive Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d88cee5",
   "metadata": {},
   "source": [
    "#### RoPE Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e41fe7",
   "metadata": {},
   "source": [
    "- Position Interpolation\n",
    "- NTK-Aware Scaling\n",
    "- xPos\n",
    "- YaRN\n",
    "- DCA: [Training-Free Long-Context Scaling of Large Language Models](https://arxiv.org/abs/2402.17463) 2024/01 (Used by Qwen2.5-1M) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feed8ac",
   "metadata": {},
   "source": [
    "### Model architecture innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed39195",
   "metadata": {},
   "source": [
    "- Sparse Attention (LongFormer, BigBird)\n",
    "- Sliding Window Attention (MPT, GPT-OSS)\n",
    "- Recurrence & Memory (Transformer-XL, Compressive Transformer, RWKV)\n",
    "- Hybrid Layer"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
