# Algorithm Building Blocks of LLMs

## Attention
Variants of Attention mechanisms, from the simplest to advanced used in popular models.
- [Scaled Dot-Product Attention](./attention/scaled_dot_product_attention/)
- [Multi-Head Attention](./attention/MHA/)
- [Multi-Query Attention](./attention/MQA/)
- [Grouped-Query Attention](./attention/GQA/)
- [Multi-Head Latent Attention](./attention/MLA/)

## Tokenizer

## Position Encoding

## Normlization

