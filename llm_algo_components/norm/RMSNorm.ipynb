{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c283038",
   "metadata": {},
   "source": [
    "# RMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e08811",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28374513",
   "metadata": {},
   "source": [
    "Deep neural networks, especially Transformers, stack dozens or even hundreds of layers. Without normalization, intermediate activations can explode (grow uncontrollably) or vanish (shrink toward zero). This leads to unstable gradients and poor convergence. Normalization keeps activations and gradients in a healthy range, preventing exploding or vanishing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a701c12",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65269ab1",
   "metadata": {},
   "source": [
    "**LayerNorm** was the normalization used in the original Transformer from 2016. It re-centers and re-scales invariance property, stabilizing training. It is also insensitive to batch size. (Ba et al. (2016). [Layer Normalization](https://arxiv.org/abs/1607.06450)). It normalizes each tokenâ€™s feature vector by re-centering to zero mean and re-scaling to unit variance, then scale and shift it with learned parameters $\\gamma$ and $\\beta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0abab6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{LN}(x)=\\gamma\\odot\\frac{x-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}+\\beta\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\mu=\\frac{1}{d}\\sum_{i=1}^d x_i,\\ \\sigma^2=\\frac{1}{d}\\sum_{i=1}^d(x_i-\\mu)^2\n",
    "$$\n",
    "\n",
    "- $\\mu$: mean of $x$\n",
    "- $\\sigma^2$: variance across $x$\n",
    "- $\\epsilon$: a tiny positive constant to avoid division-by-zero "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df9894",
   "metadata": {},
   "source": [
    "Edit: [gist](https://gist.github.com/furixturi/0ad389170baec66df12b2a31ee7e13f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907e67c",
   "metadata": {},
   "source": [
    "Sample implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f43ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d))\n",
    "        self.beta  = nn.Parameter(torch.zeros(d))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):  # x: [..., d]\n",
    "        mu = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, unbiased=False, keepdim=True)\n",
    "        xhat = (x - mu) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * xhat + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccfe182",
   "metadata": {},
   "source": [
    "## RMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42753da2",
   "metadata": {},
   "source": [
    "RMSNorm was introduced in 2019 (B. Zhang, R. Sennrich [Root Mean Square Layer Normalization](https://arxiv.org/pdf/1910.07467)). It removed mean subtraction, scaling only by root-mean-square. The hypothesis is that LayerNorm's success is mainly attributed to re-scaling invariance rather than re-centering. The quality is similar to LayerNorm and the computation is slightly cheaper/faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11987b",
   "metadata": {},
   "source": [
    "LLaMA-family adopted and popularized RMSNorm and it becomes the de facto normalization of modern LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ed336",
   "metadata": {},
   "source": [
    "Formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ff1ce",
   "metadata": {},
   "source": [
    "$$\n",
    "RN(x_i) = \\frac{x_i}{RMS(x)}g_i, \\quad \\text{RMS}(a) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n a_i^2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x \\in \\mathbb{R}^n$ is the input vector\n",
    "- $g \\in \\mathbb{R}^n$ is a learned scaling parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb304356",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52389fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(d))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt((x * x).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        return self.g * (x / rms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
