{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a96464",
   "metadata": {},
   "source": [
    "# Multi-head Latent Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7369654",
   "metadata": {},
   "source": [
    "Multi-head Latent Attention (MLA) is a variant of MHA that drastically reduce the memory footprint and compute cost of th KV cache in LLMs in inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ea70f",
   "metadata": {},
   "source": [
    "Traditional MHA caches large K and V matrices for each token, which grows the cache size quadratically as the sequence length grows and becomes a major bottleneck for long contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74c080",
   "metadata": {},
   "source": [
    "MLA addresses this with low-rank compression, projecting the input hidden state into a much smaller latent space. It introduces a down-projection layer that compresses the large K and V matrices into a single, much smaller latent representation (a \"latent KV\" matrix) and stores only this small latent representation in the KV cache. At attention computation, this latent matrix is then \"up-projected\" by specific per-head linear layers to reconstruct the K and V vectors in their respective head dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567904e",
   "metadata": {},
   "source": [
    "It was introduced in DeepSeek-v2 paper [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434), where in the ablation tests they found that MLA even perform better than the traditional MHA. MLA is also used in DeepSeek-v3 and DeepSeek R1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51470cc3",
   "metadata": {},
   "source": [
    "MLA pairs especially well with KV-Cache at inference time by greatly reducing KV cache memory footprint. The inference memory efficiency makes MLA suitable for scenarios where inference speed is critical or memory is constraint, e.g.:\n",
    "- long-context LLM (without hitting memory limits)\n",
    "- edge and mobile devices\n",
    "- efficient inference servers (serve faster and more users on a single GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7c1bf",
   "metadata": {},
   "source": [
    "MLA often incorporates a \"decoupled\" RoPE. As the standard RoPE directly modifies K and V, in MLA, applying RoPE direcly on compressed K and V can be problematic or inefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f367d6e",
   "metadata": {},
   "source": [
    "The full MLA formula as proposed in DeepSeek-v2 paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6ce2b",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "{c}_t^{Q} &= W^{DQ} {h}_t, \\tag{1} \\\\\n",
    "\\left[ \\mathbf{q}_{t,1}^{C}; \\mathbf{q}_{t,2}^{C}; \\ldots; \\mathbf{q}_{t,n_h}^{C} \\right] &= \\mathbf{q}_t^{C} = W^{UQ} \\mathbf{c}_t^{Q}, \\tag{2} \\\\\n",
    "\\left[ \\mathbf{q}_{t,1}^{R}; \\mathbf{q}_{t,2}^{R}; \\ldots; \\mathbf{q}_{t,n_h}^{R} \\right] &= \\mathbf{q}_t^{R} = \\mathrm{RoPE} \\left( W^{QR} \\mathbf{c}_t^{Q} \\right), \\tag{3} \\\\\n",
    "\\mathbf{q}_{t,i} &= \\left[ \\mathbf{q}_{t,i}^{C}; \\mathbf{q}_{t,i}^{R} \\right], \\tag{4} \\\\\n",
    "\\mathbf{c}_t^{KV} &= W^{DKV} \\mathbf{h}_t, \\tag{5} \\\\\n",
    "\\left[ \\mathbf{k}_{t,1}^{C}; \\mathbf{k}_{t,2}^{C}; \\ldots; \\mathbf{k}_{t,n_h}^{C} \\right] &= \\mathbf{k}_t^{C} = W^{UK} \\mathbf{c}_t^{KV}, \\tag{6} \\\\\n",
    "\\mathbf{k}_t^{R} &= \\mathrm{RoPE} \\left( W^{KR} \\mathbf{h}_t \\right), \\tag{7} \\\\\n",
    "\\mathbf{k}_{t,i} &= \\left[ \\mathbf{k}_{t,i}^{C}; \\mathbf{k}_{t,i}^{R} \\right], \\tag{8} \\\\\n",
    "\\left[ \\mathbf{v}_{t,1}^{C}; \\mathbf{v}_{t,2}^{C}; \\ldots; \\mathbf{v}_{t,n_h}^{C} \\right] &= \\mathbf{v}_t^{C} = W^{UV} \\mathbf{c}_t^{KV}, \\tag{9} \\\\\n",
    "o_{t,i} &= \\sum_{j=1}^{t} \\mathrm{Softmax}_j \\left( \\frac{ \\mathbf{q}_{t,i}^\\top \\mathbf{k}_{j,i} }{ \\sqrt{d_h + d_h^{R}} } \\right) \\mathbf{v}_{j,i}^{C}, \\tag{10} \\\\\n",
    "\\mathbf{u}_t &= W^{O} \\left[ o_{t,1}; o_{t,2}; \\ldots; o_{t,n_h} \\right] \\tag{11}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130df57",
   "metadata": {},
   "source": [
    "Where:\n",
    "- $h_t$: input token embedding at position $t$\n",
    "- $n_h$: number of attention heads\n",
    "- $W^{DQ}, W^{DKV}$: down-projection matrices for query and key-value content vectors\n",
    "- $W^{UQ}, W^{UK}, W^{UV}$: up-projection for query, key and value from content vectors\n",
    "- $W^{QR}, W^{KR}$: linear projections generating relative queries and keys (before RoPE)\n",
    "- $W^O$: output linear projection matrix\n",
    "- $c_t^Q$: content query vector (down-projected from input $h_t$)\n",
    "- $c_t^{KV}$: content key-value vector (also down-projected from input)\n",
    "- $q_t^C, q_{t,i}^C$: content queries of all heads / head i\n",
    "- $q_t^R, q_{r,i}^R$: relative positional queries of all heads / head i\n",
    "- $q_{t,i}$: concatenated content and relative query vectors of head i\n",
    "- $k_t^C, k_{t,i}^C$: content keys for all heads / head i\n",
    "- $k_t^R$: relative positional keys\n",
    "- $k_{t,i}$: concatenated content and relative key vectors of head i\n",
    "- $v_t^C, v_{t,i}^C$: content values for all heads / head i\n",
    "- $d_h, d_h^R$: dimensions of content and relative positional subspaces per head\n",
    "- $o_{t,i}$: attention output for head i at position t\n",
    "- $u_t$: final output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3550d",
   "metadata": {},
   "source": [
    "Line-by-line explanation:\n",
    "1. Compress input embedd $h_t$ into query latent space $c_t^Q$\n",
    "2. Decompress latent query $c_t^Q$ back to full dimension $q_t^C$ and split across $n_h$ heads for multi-head attention\n",
    "3. Generate relative positional queries $q_t^R$ from compressed query $c_t^Q$, apply $RoPE$ to handle positional information separately from content (decoupled RoPE)\n",
    "4. Concatenate each head's queries $q_{t,i}^C$ and their relative positions $q_{t,i}^R$\n",
    "5. Compress inputs $h_t$ into a shared key and value latent content space $c_t^{KV}$ - *this will be cached*\n",
    "6. Decompress latent shared content $c^{KV}$ to keys for all heads $k_t^C$\n",
    "7. Generate relative positional keys $k_t^R$  directly from input $h_t$ to apply $RoPE$ - *this will be cached*\n",
    "8. Concatenate each head's keys $k_{t,i}^C$ and their relative positions $k_{t,i}^R$\n",
    "9. Decompress latent shared content $c^{KV}$ to values for all heads $v_t^C$, values only need content as they don't have positional content\n",
    "10. Compute scaled dot-product attention \n",
    "11. Compute final attention output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8c040",
   "metadata": {},
   "source": [
    "The optimization as directly quoted from the paper:\n",
    "\n",
    "$c_t^{KV}$ and $k_t^R$ will be cached for generation. During inference, the naive formula needs to recover $k_t^C$ and $v_t^C$ from $c_t^{KV}$ for attention. Fortunately, due to the associative law of matrix multiplication, we can absorb $W^{UK}$ into $W^{UQ}$, and $W^{UV}$ into $W^O$. Therefore, we do not need to compute keys and values out for each query. Through this optimization, we avoid he computational overhead for recomputing $k_t^C$ and $v_t^C$ during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64453d92",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2bf486",
   "metadata": {},
   "source": [
    "Simplified implementation following the formula but without RoPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a2567fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MaskedScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        d_k = queries.size(-1)\n",
    "        attn_scores = torch.matmul(queries, keys.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        attn_values = torch.matmul(attn_weights, values)\n",
    "        return attn_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93da377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Latent Attention (MLA) following DeepSeek-v2 formula.\n",
    "    \n",
    "    Core idea: Separate compression of Q and KV, with a shared KV compression.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, q_latent_dim, kv_latent_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, f'Embedding dimension must be divisible by number of heads'\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.q_latent_dim = q_latent_dim\n",
    "        self.kv_latent_dim = kv_latent_dim\n",
    "\n",
    "        # Initialize down-projection and up-projection matrices\n",
    "        self.W_DQ = nn.Linear(embed_dim, q_latent_dim)\n",
    "        self.W_UQ = nn.Linear(q_latent_dim, embed_dim)\n",
    "        self.W_DKV = nn.Linear(embed_dim, kv_latent_dim)\n",
    "        self.W_UK = nn.Linear(kv_latent_dim, embed_dim)\n",
    "        self.W_UV = nn.Linear(kv_latent_dim, embed_dim)\n",
    "        # Initialize final output \n",
    "        self.W_output = nn.Linear(embed_dim, embed_dim) \n",
    "\n",
    "        self.attention = MaskedScaledDotProductAttention(dropout)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "\n",
    "        # Step 1. Compress and decompress Q\n",
    "        c_Q = self.W_DQ(x) # (batch_size, seq_len, q_latent_dim)\n",
    "        q_content = self.W_UQ(c_Q) # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Step 2. Compress KV into c_KV\n",
    "        c_KV = self.W_DKV(x) # (batch_size, seq_len, kv_latent_dim)\n",
    "\n",
    "        # Step 3. Decompress K and V\n",
    "        k_content = self.W_UK(c_KV) # (batch_size, seq_len, embed_dim)\n",
    "        v_content = self.W_UV(c_KV) # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Step 4. Reshape for multi-head attention -> (batch_size, num_heads, seq_len, head_dim)\n",
    "        queries = q_content.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        keys = k_content.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        values = v_content.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # Step 5. Apply attention\n",
    "        attn_output = self.attention(queries, keys, values, mask) # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Step 6. Concatenate attention heads and reshape\n",
    "        attn_output = attn_output.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Step 7. Apply final output projection\n",
    "        output = self.W_output(attn_output)\n",
    "\n",
    "        return output, c_KV # return c_KV to cache it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d387a",
   "metadata": {},
   "source": [
    "### Test and Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e104fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 1024, 512])\n",
      "Output shape: torch.Size([2, 1024, 512])\n",
      "Latent KV cache shape: torch.Size([2, 1024, 128])\n",
      "\n",
      "Memory savings analysis (assuming float16/bfloat16 precision):\n",
      "Standard MHA KV cache per layer: 2.00 MB\n",
      "MLA latent cache per layer: 0.25 MB\n",
      "Memory reduction: 8.0x smaller\n",
      "\n",
      "(For reference - float32 would be: 4.00 MB for MHA)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Model parameters\n",
    "    embed_dim = 512\n",
    "    num_heads = 8\n",
    "    q_latent_dim = 128  # Much smaller than embed_dim for memory savings\n",
    "    kv_latent_dim = 128\n",
    "    seq_len = 1024\n",
    "    batch_size = 2\n",
    "    \n",
    "    # Create model and input\n",
    "    mla = MultiHeadLatentAttention(embed_dim, num_heads, q_latent_dim, kv_latent_dim)\n",
    "    x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, c_kv = mla(x)\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Latent KV cache shape: {c_kv.shape}\")\n",
    "    \n",
    "    # Memory comparison (typical inference uses float16/bfloat16, not float32)\n",
    "    bytes_per_element = 2  # float16/bfloat16 (common in inference)\n",
    "    print(f\"\\nMemory savings analysis (assuming float16/bfloat16 precision):\")\n",
    "    print(f\"Standard MHA KV cache per layer: {2 * seq_len * embed_dim * bytes_per_element / 1024**2:.2f} MB\")\n",
    "    print(f\"MLA latent cache per layer: {seq_len * kv_latent_dim * bytes_per_element / 1024**2:.2f} MB\") \n",
    "    print(f\"Memory reduction: {(2 * embed_dim) / kv_latent_dim:.1f}x smaller\")\n",
    "    \n",
    "    # For comparison - float32 would be 2x larger\n",
    "    print(f\"\\n(For reference - float32 would be: {2 * seq_len * embed_dim * 4 / 1024**2:.2f} MB for MHA)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
