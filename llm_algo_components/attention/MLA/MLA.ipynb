{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a96464",
   "metadata": {},
   "source": [
    "# Multi-head Latent Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7369654",
   "metadata": {},
   "source": [
    "Multi-head Latent Attention (MLA) is a variant of MHA that drastically reduce the memory footprint and compute cost of th KV cache in LLMs in inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ea70f",
   "metadata": {},
   "source": [
    "Traditional MHA caches large K and V matrices for each token, which grows the cache size quadratically as the sequence length grows and becomes a major bottleneck for long contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74c080",
   "metadata": {},
   "source": [
    "MLA addresses this with low-rank compression, projecting the input hidden state into a much smaller latent space. It introduces a down-projection layer that compresses the large K and V matrices into a single, much smaller latent representation (a \"latent KV\" matrix) and stores only this small latent representation in the KV cache. At attention computation, this latent matrix is then \"up-projected\" by specific per-head linear layers to reconstruct the K and V vectors in their respective head dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567904e",
   "metadata": {},
   "source": [
    "It was introduced in DeepSeek-v2 paper [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434), where in the ablation tests they found that MLA even perform better than the traditional MHA. MLA is also used in DeepSeek-v3 and DeepSeek R1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51470cc3",
   "metadata": {},
   "source": [
    "MLA pairs especially well with KV-Cache at inference time by greatly reducing KV cache memory footprint. The inference memory efficiency makes MLA suitable for scenarios where inference speed is critical or memory is constraint, e.g.:\n",
    "- long-context LLM (without hitting memory limits)\n",
    "- edge and mobile devices\n",
    "- efficient inference servers (serve faster and more users on a single GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7c1bf",
   "metadata": {},
   "source": [
    "MLA often incorporates a \"decoupled\" RoPE. As the standard RoPE directly modifies K and V, in MLA, applying RoPE direcly on compressed K and V can be problematic or inefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64453d92",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5383b08",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
