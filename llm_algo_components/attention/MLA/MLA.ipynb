{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a96464",
   "metadata": {},
   "source": [
    "# Multi-head Latent Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7369654",
   "metadata": {},
   "source": [
    "Multi-head Latent Attention (MLA) is a variant of MHA that drastically reduce the memory footprint and compute cost of th KV cache in LLMs in inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ea70f",
   "metadata": {},
   "source": [
    "Traditional MHA caches large K and V matrices for each token, which grows the cache size quadratically as the sequence length grows and becomes a major bottleneck for long contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74c080",
   "metadata": {},
   "source": [
    "MLA addresses this with low-rank compression, projecting the input hidden state into a much smaller latent space. It introduces a down-projection layer that compresses the large K and V matrices into a single, much smaller latent representation (a \"latent KV\" matrix) and stores only this small latent representation in the KV cache. At attention computation, this latent matrix is then \"up-projected\" by specific per-head linear layers to reconstruct the K and V vectors in their respective head dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567904e",
   "metadata": {},
   "source": [
    "It was introduced in DeepSeek-v2 paper [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434), where in the ablation tests they found that MLA even perform better than the traditional MHA. MLA is also used in DeepSeek-v3 and DeepSeek R1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51470cc3",
   "metadata": {},
   "source": [
    "MLA pairs especially well with KV-Cache at inference time by greatly reducing KV cache memory footprint. The inference memory efficiency makes MLA suitable for scenarios where inference speed is critical or memory is constraint, e.g.:\n",
    "- long-context LLM (without hitting memory limits)\n",
    "- edge and mobile devices\n",
    "- efficient inference servers (serve faster and more users on a single GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7c1bf",
   "metadata": {},
   "source": [
    "MLA often incorporates a \"decoupled\" RoPE. As the standard RoPE directly modifies K and V, in MLA, applying RoPE direcly on compressed K and V can be problematic or inefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64453d92",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Latent Attention (MLA) - A memory-efficient attention mechanism.\n",
    "    \n",
    "    Core idea: Compress K,V matrices into a smaller latent space to reduce KV cache size.\n",
    "    Instead of caching full K,V matrices, we cache only the compressed representation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, latent_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Standard query projection\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "        # MLA core: compress K,V into latent space, then decompress per head\n",
    "        self.kv_compress = nn.Linear(embed_dim, latent_dim, bias=False)\n",
    "        self.k_decompress = nn.Linear(latent_dim, embed_dim, bias=False)\n",
    "        self.v_decompress = nn.Linear(latent_dim, embed_dim, bias=False)\n",
    "        \n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor [batch, seq_len, embed_dim]\n",
    "            mask: Optional attention mask [batch, seq_len, seq_len] or broadcastable\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # 1. Compress input into latent space (this is what gets cached in inference)\n",
    "        latent = self.kv_compress(x)  # [batch, seq_len, latent_dim]\n",
    "        \n",
    "        # 2. Generate Q, K, V\n",
    "        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_decompress(latent).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_decompress(latent).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 3. Scaled dot-product attention\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # 4. Apply attention to values\n",
    "        out = torch.matmul(attn_weights, V)  # [batch, num_heads, seq_len, head_dim]\n",
    "        \n",
    "        # 5. Concatenate heads and project\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        return self.out_proj(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d387a",
   "metadata": {},
   "source": [
    "### Test and Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e104fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 1024, 512])\n",
      "Output shape: torch.Size([2, 1024, 512])\n",
      "\n",
      "Memory savings analysis (assuming float16/bfloat16 precision):\n",
      "Standard MHA KV cache per layer: 2.00 MB\n",
      "MLA latent cache per layer: 0.25 MB\n",
      "Memory reduction: 8.0x smaller\n",
      "\n",
      "(For reference - float32 would be: 4.00 MB for MHA)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Model parameters\n",
    "    embed_dim = 512\n",
    "    num_heads = 8\n",
    "    latent_dim = 128  # Much smaller than embed_dim for memory savings\n",
    "    seq_len = 1024\n",
    "    batch_size = 2\n",
    "    \n",
    "    # Create model and input\n",
    "    mla = MultiHeadLatentAttention(embed_dim, num_heads, latent_dim)\n",
    "    x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = mla(x)\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # Memory comparison (typical inference uses float16/bfloat16, not float32)\n",
    "    bytes_per_element = 2  # float16/bfloat16 (common in inference)\n",
    "    print(f\"\\nMemory savings analysis (assuming float16/bfloat16 precision):\")\n",
    "    print(f\"Standard MHA KV cache per layer: {2 * seq_len * embed_dim * bytes_per_element / 1024**2:.2f} MB\")\n",
    "    print(f\"MLA latent cache per layer: {seq_len * latent_dim * bytes_per_element / 1024**2:.2f} MB\") \n",
    "    print(f\"Memory reduction: {(2 * embed_dim) / latent_dim:.1f}x smaller\")\n",
    "    \n",
    "    # For comparison - float32 would be 2x larger\n",
    "    print(f\"\\n(For reference - float32 would be: {2 * seq_len * embed_dim * 4 / 1024**2:.2f} MB for MHA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3628d2e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
