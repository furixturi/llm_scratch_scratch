{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de79829f",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3192cc47",
   "metadata": {},
   "source": [
    "An implementation of Multi-Head Attention (MHA) with learnable parameters for query, key, value, and final output to combine heads (W_q, W_k, W_v, W_o)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b486f49",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5465be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc3e60d",
   "metadata": {},
   "source": [
    "We reuse the [scaled dot-product attention](../scaled_dot_product_attention/) implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c767eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=query.dtype))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Learnable parameters (linear projections) for query, key, value, and output\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim) # W_q for query\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim) # W_k for key\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim) # W_v for value\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim) # W_o to learn how to combine each head's output into a final output\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Pass the input X through the Q, K, V parameter matrices, \n",
    "        # then reshape them to multi-heads (batch_size, seq_len, num_heads, head_dim)\n",
    "        # and transpose to (batch_size, num_heads, seq_len, head_dim) to apply attention for each head in parallel\n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  \n",
    "        k = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  \n",
    "        v = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Apply scaled dot-product attention to each head\n",
    "        attn_output, attn_weights = self.attention(q, k, v, mask)\n",
    "\n",
    "        # Concatenate each head's output\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        # Pass the concatenated output through the final projection (learnable paramters to combine meanings of each head's output)\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae32d2",
   "metadata": {},
   "source": [
    "## Line-by-line Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d04de",
   "metadata": {},
   "source": [
    "### def \\_\\_init\\_\\_(self, embed_dim, num_heads, dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd5d2ae",
   "metadata": {},
   "source": [
    "- Make sure embedding dimension is divisible by number of heads\n",
    "  - `assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"`\n",
    "- Store the embedding dimension, number of heads, and head dimension as instance attributes\n",
    "  - ```\n",
    "    self.embed_dim = embed_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = embed_dim // num_heads\n",
    "    ```\n",
    "- Create weight matrices for Q, K, V and head combination output\n",
    "  - ```\n",
    "    self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    ```\n",
    "- Store the scaled dot-product computation nn.Module also as an instance attribute for later usage\n",
    "  - `self.attention = ScaledDotProductAttention(dropout)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23533de8",
   "metadata": {},
   "source": [
    "### def forward(self, x, mask=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b27ef2",
   "metadata": {},
   "source": [
    "- Get batch size and sequence length from input tensor X, which are the first two dimensions of x's shape\n",
    "  - `batch_size, seq_len, _ = x.shape` \n",
    "- Pass the input X through the query, key, value weight matrices, then split the last dimension `emb_dim` into `(num_head, head_dim)`. Then transpose the second and third dimensions so that we have them in shape `(batch_size, num_head, seq_len, head_dim)` for parallel multi-head attention computation\n",
    "  - ```\n",
    "    q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    k = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    v = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    ```\n",
    "- Apply attention calculation to each head\n",
    "  - `attn_output, attn_weights = self.attention(q, k, v, mask)`\n",
    "    - Thanks to PyTorchâ€™s batch-aware `matmul` and `broadcasting` mechanisms, this one line will apply attention to all heads at once in parallel over the `num_heads` dimension, which is more efficient than using an explicit for-loop over heads.\n",
    "- Concatenate the output of each head\n",
    "  - `attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)`\n",
    "    - First, transpose back the `num_head` and `seq_len` dimensions\n",
    "    - Use `.contiguous()` to ensure the tensor's memory layout is row major and contiguous in memory, which is required by `.view()` to reshape them\n",
    "    - Use `.view()` to merge the last two dimensions `num_head` and `head_dim` into the original `self.embed_dim`\n",
    "- Pass the concatenated output through the output weight matrix to combine each head meaningfully into the final output\n",
    "  - `output = self.out_proj(attn_output)`\n",
    "- Return the final output and attention weights\n",
    "  - `return output, attn_weights`\n",
    "    - `output` is of shape `(batch_size, seq_len, embed_dim)`, ready to be passed to the next layer\n",
    "    - `attn_weights` is the attention weights per head, so its shape is `(batch_size, num_heads, seq_len, seq_len)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91669734",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \\\n",
    "            f'Model embedding dimension (received embed_dim: \\\n",
    "                {embed_dim}) must be divisible by number of \\\n",
    "                    attention heads (received num_heads: {num_heads})'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim / num_heads\n",
    "\n",
    "        # initialize Q, K, V, and output linear projection layers\n",
    "        # input x (batch_size, seq_len, embed_dim) will go through them\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # initialize dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (batch_size, seq_len, embed_dim)\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "\n",
    "        # Step 1: Project input through the Q, K, V linear projections\n",
    "        #       shape of x: (batch_size, seq_len, embed_dim)\n",
    "        #       shape of W_q, W_k, W_v: (embed_dim, embed_dim)\n",
    "        #       shape after projection: (batch_size, seq_len, embed_dim)\n",
    "        # Step 2: Split the last embedding dimension into multiple heads\n",
    "        #       shape after split: (batch_size, seq_len, num_heads, head_dim)\n",
    "        # Step 3: Transpose the seq_len and num_heads dimensions for \n",
    "        #           parallel attention computation\n",
    "        #       shape after transpose: (batch_size, num_heads, seq_len, head_dim)\n",
    "        queries = self.W_q(x).view(batch_size, seq_len, self.num_heads, \\\n",
    "                                   self.head_dim).transpose(1,2)\n",
    "        keys = self.W_k(x).view(batch_size, seq_len, self.num_heads, \\\n",
    "                                self.head_dim).transpose(1,2)\n",
    "        values = self.W_v(x).vies(batch_size, seq_len, self.num_heads, \\\n",
    "                                  self.head_dim).transpose(1,2)\n",
    "\n",
    "        # Step 4: Compute attention output of each head\n",
    "        # Step 4-1: attention score computation\n",
    "        #       shapes: \n",
    "        #           queries: (batch_size, num_heads, seq_len, head_dim)\n",
    "        #           keys.transpose(): (batch_size, num_heads, head_dim, seq_len)\n",
    "        #           attn_scores: (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn_scores = torch.matmul(queries, keys.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
    "        # Step 4-2: apply mask\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        # Step 4-3: softmax\n",
    "        attn_weights = F.softmax(attn_scores, dim = -1)\n",
    "        # Step 4-4: dropout\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        # Step 4-5: attention value of each head\n",
    "        #       shapes:\n",
    "        #           attn_weights: (batch_size, num_heads, seq_len, seq_len)\n",
    "        #           values: (batch_size, num_heads, seq_len, head_dim)\n",
    "        #           attn_values: (batch_size, num_heads, seq_len, head_dim)\n",
    "        attn_values = torch.matmul(attn_weights, values)\n",
    "\n",
    "        # Step 5: rearrange dimensions and reshape to concatenate heads\n",
    "        #       shapes:\n",
    "        #           before rearrange: (batch_size, num_heads, seq_len, head_dim)\n",
    "        #           after rearrange: (batch_size, seq_len, num_heads, head_dim)\n",
    "        #           after concatenation: (batch_size, seq_len, embed_dim)\n",
    "        attn_values = attn_values.transpose(1, 2).reshape(batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Step 6: Go through the final output projection\n",
    "        output = self.W_out(attn_values)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
