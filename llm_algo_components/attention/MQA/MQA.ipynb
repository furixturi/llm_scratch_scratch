{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "448609b3",
   "metadata": {},
   "source": [
    "# Multi-Query Attention (MQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b3bb96",
   "metadata": {},
   "source": [
    "Multi-Query Attention (MQA) is a way to improve Multi-Head Attention (MHA) inference efficiency by sharing the keys and values across all attention heads. It greatly reduces memory bandwidth required for loading key and value tensors over and over again during incremental decoding, at the cost of only a minor quality degradation. \n",
    "\n",
    "It was introduced in Google's 2019 paper \"Fast Transformer Decoding: One Write-Head is All You Need\". [arxiv.org/pdf/1911.02150](https://arxiv.org/pdf/1911.02150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba8ff3",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47bec406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7894e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=query.dtype))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53463885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_query_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_query_heads == 0, \"embed_dim must be divisible by num_query_heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_query_heads = num_query_heads\n",
    "        self.head_dim = embed_dim // num_query_heads # per head dimension\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim) # multiple query heads\n",
    "        self.k_proj = nn.Linear(embed_dim, self.head_dim) # shared key head\n",
    "        self.v_proj = nn.Linear(embed_dim, self.head_dim) # shared value head\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim) # output projection\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, embed_dim) - input token representations\n",
    "            mask: (batch_size, num_query_heads, seq_len, seq_len) - optional attention mask\n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, embed_dim)\n",
    "            attn_weights: (batch_size, num_query_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Project input x through query linear projection and reshape for multi-query heads\n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.num_query_heads, self.head_dim).transpose(1, 2)\n",
    "        # Project input x through shared key and value linear projections, then broadcast to share across query heads\n",
    "        k = self.k_proj(x).unsqueeze(1) # (batch_size, 1, seq_len, head_dim)\n",
    "        v = self.v_proj(x).unsqueeze(1) # (batch_size, 1, seq_len, head_dim)\n",
    "\n",
    "        # Multi-head Scaled dot-product attention. attn_output: (batch_size, num_query_heads, seq_len, head_dim)\n",
    "        attn_output, attn_weights = self.attention(q, k, v, mask)\n",
    "        # (batch_size, num_query_heads, seq_len, head_dim) -> (batch_size, seq_len, num_query_heads, head_dim) \n",
    "        # -> reshape to (batch_size, seq_len, embed_dim)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "\n",
    "        # Final output projection: (batch_size, seq_len, embed_dim)\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa7656",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11980feb",
   "metadata": {},
   "source": [
    "Most parts are the same as the [multi head attention (MHA)](../MHA/MHA.ipynb). We just pick up the differences here for a brief explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d906ea33",
   "metadata": {},
   "source": [
    "### def \\_\\_init\\_\\_(self, embed_dim, num_query_heads, dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedac6e5",
   "metadata": {},
   "source": [
    "- Create weight matrices (linear projections) differently for Q and K/V so that Q will be split into multiple query heads, K/V will be shared across these query heads. Specifically, the last dimension of Q projection is of full embedding dimension, while that of K and V is of each head's embedding dimension from `self.head_dim = embed_dim/num_query_head`\n",
    "  - ```\n",
    "    self.q_proj = nn.Linear(embed_dim, embed_dim) # multiple query heads\n",
    "    self.k_proj = nn.Linear(embed_dim, self.head_dim) # shared key head\n",
    "    self.v_proj = nn.Linear(embed_dim, self.head_dim) # shared value head\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e325c2",
   "metadata": {},
   "source": [
    "### def forward(self, x, mask=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e2b34",
   "metadata": {},
   "source": [
    "- Project x through Q linear projection layer, then split and reshape it for multi-head batching is the same as MHA\n",
    "- Project x through K and V linear projection layers. Since these two are to be shared and their sizes are only of a single head, we don't split them but add an extra head dimension with value `1` to match the multi-head Q tensor. This enables **broadcasting**\n",
    "  - ```\n",
    "    k = self.k_proj(x).unsqueeze(1) # (batch_size, 1, seq_len, head_dim)\n",
    "    v = self.v_proj(x).unsqueeze(1) # (batch_size, 1, seq_len, head_dim)\n",
    "    ```\n",
    "- Everything else works the same as MHA.\n",
    "\n",
    "Notes about the mask shape:\n",
    "- For the mask to be working with MQA, it needs to either be able to broadcast, or match the shape of the multi query head. This is the same as MHA. \n",
    "  - For decoder-only Transformer the attention is autoregressive (self-attention, no cross-attention), so the sequence length of query (query_len) and key (key_len) are the same and we can name them both `seq_len`. Here we differentiate the names just for clarity.\n",
    "- In the case of matching the shape, the mask would be in shape \n",
    "  - `(batch_size, num_query_head, query_len, key_len)`\n",
    "- In the case of broadcast, it could be one of these\n",
    "  - Causal mask (same for all batches/heads)\n",
    "    - `(1, 1, query_len, key_len)`\n",
    "  - Padding mask per token\n",
    "    - `(batch_size, 1, 1, key_len)`\n",
    "  - Fully customized\n",
    "    - `(batch_size, num_query_head, query_len, key_len)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14af8b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
