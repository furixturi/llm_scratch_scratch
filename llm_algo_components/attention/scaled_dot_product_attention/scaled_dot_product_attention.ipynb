{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56db8b3",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b2c041",
   "metadata": {},
   "source": [
    "An implementation of scaled dot-product attention with PyTorch, optionally accepting a mask for causal attention. This itself is not a learnable part but is a key computation used in Transformers' attention layers' MHA components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27dfbde",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec4efc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa35a801",
   "metadata": {},
   "source": [
    "## Line-by-Line Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024898d0",
   "metadata": {},
   "source": [
    "### def \\_\\_init\\_\\_(self, dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a84cec",
   "metadata": {},
   "source": [
    "- First, instantiate the parent class `nn.Module` \n",
    "\n",
    "- `self.dropout = nn.Dropout(dropout)`: Initialize a Dropout layer to apply dropout to attention weights, where we randomly drop some attention scores. \n",
    "  - Why: Adding dropout helps prevent overfitting and better generalize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf49335",
   "metadata": {},
   "source": [
    "### def forward(self, query, key, value, mask=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc33954",
   "metadata": {},
   "source": [
    "This function implements the scaled dot-product attention formula below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee6e103",
   "metadata": {},
   "source": [
    "$\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left( \\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3737fad",
   "metadata": {},
   "source": [
    "The input tensors:\n",
    "- query, key, value are of the same shape: `(batch_size, seq_len, d_k)`\n",
    "  - `d_k` is the dimensionality of the query and key vectors. In single-head attention implementations, it equals `d_model`, aka the embedding size. In multi-head attention, it equals to `d_model / num_head`\n",
    "- mask shape is `(batch_size, seq_len, seq_len)`\n",
    "  - it's a binary mask, 1 = attend, 0 = ignore. We use it to ignore future tokens or padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90649878",
   "metadata": {},
   "source": [
    "Line-by-line:\n",
    "- Get the scale dimension: \n",
    "  - `d_k = query.size(-1)`  \n",
    "    - This is used for **scaling** the dot product later.\n",
    "- Compute attention scores: \n",
    "  - `scores = torch.matmul(query, key.transpose(-2, -1))/torch.sqrt(torch.tensor(d_k, dtype=query.dtype))` \n",
    "    - First, compute the dot products of `query @ keyᵀ` which indicate how much attention one position should pay to another\n",
    "    - Then, scale the dot products (divide by √d_k) to **scale down large values** and stablize softmax. This was recommended in the original Transformer paper\n",
    "- If mask was provided in the input, apply it\n",
    "  - `scores = scores.masked_fill(mask == 0, float('-inf'))` \n",
    "    - The mask is a binary mask with 1 for attend and 0 for ignore. Here we replace positions with 0 value with `-inf` in the mask, so that softmax scores them to zero, effectively paying zero attention to them.\n",
    "- Convert attention scores to attention weights (probability distribution):\n",
    "  - `attention_weights = F.softmax(scores, dim=-1)`  \n",
    "    - All weights then fall into the range of 0 and 1 and sum up to 1.\n",
    "- Apply dropout: \n",
    "  - `attention_weights = self.dropout(attention_weights)`  \n",
    "    - Apply dropout to attention weights regularizes the model by randomly zeroing some weights. This prevents model from depending too heavily on any one token during trianing, thus improves model robustness and generalization.\n",
    "- Calculate output, which is a weighted sum of the value vectors using the attention weights: \n",
    "  - `output = torch.matmul(attention_weights, value)` \n",
    "    - This combines information from all tokens (their values) according to how much attention should be paid to each of them\n",
    "    - output shape is also `(batch_size, seq_len, d_k)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f26f690",
   "metadata": {},
   "source": [
    "The output tensors:\n",
    "- output: context-aware information from all tokens. Shape: `(batch_size, seq_len, d_k)`\n",
    "  - This is used as input to the next layer. It is also used in backpropagation during training.\n",
    "- attention_weights: distribution of attention over all tokens. Shape: `(batch_size, seq_len, seq_len)` \n",
    "  - This shows attention distribution and can be used in visualization / interpretability analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83281f72",
   "metadata": {},
   "source": [
    "Notes\n",
    "- The order of applying mask, softmax, dropout is in the order \n",
    "  - mask -> softmax -> dropout\n",
    "- Mask is applied before Softmax, because\n",
    "  - \n",
    "- Softmax is applied before Dropout, because \n",
    "  - We want to randomly zero out token contributions (probabilities), not distort the probability distribution before calculating it from the similarity scores (softmax). \n",
    "  - This will result in the attention weights not sum up to 1 anymore but it is intentional during training. Re-normalize after dropout is also possible but not widely used. \n",
    "  - At inference, the dropout is disabled so that the token probability distribution sum up to 1. \n",
    "    - This is realized through using PyTorch's `nn.Dropout` module for dropout. Internally PyTorch tracks whether the model is in training `model.train()`or inference mode `model.eval()`. Dropout is then only applied for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976b966",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e760997f",
   "metadata": {},
   "source": [
    "## Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee035796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Basic Functionality ===\n",
      "✓ Basic functionality test passed!\n",
      "  Output shape: torch.Size([2, 4, 8])\n",
      "  Attention weights shape: torch.Size([2, 4, 4])\n",
      "  Attention weights sum: 1.000000 (should be ~1.0)\n"
     ]
    }
   ],
   "source": [
    "def test_basic_functionality():\n",
    "    \"\"\"Test basic functionality of ScaledDotProductAttention\"\"\"\n",
    "    print(\"=== Test 1: Basic Functionality ===\")\n",
    "    \n",
    "    # Setup\n",
    "    batch_size, seq_len, d_k = 2, 4, 8\n",
    "    attention_layer = ScaledDotProductAttention(dropout=0.0)  # No dropout for deterministic test\n",
    "    \n",
    "    # Create test inputs\n",
    "    query = torch.randn(batch_size, seq_len, d_k)\n",
    "    key = torch.randn(batch_size, seq_len, d_k)\n",
    "    value = torch.randn(batch_size, seq_len, d_k)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attention_weights = attention_layer(query, key, value)\n",
    "    \n",
    "    # Test output shapes\n",
    "    assert output.shape == (batch_size, seq_len, d_k), f\"Expected output shape {(batch_size, seq_len, d_k)}, got {output.shape}\"\n",
    "    assert attention_weights.shape == (batch_size, seq_len, seq_len), f\"Expected attention shape {(batch_size, seq_len, seq_len)}, got {attention_weights.shape}\"\n",
    "    \n",
    "    # Test attention weights properties\n",
    "    # 1. All weights should be non-negative\n",
    "    assert torch.all(attention_weights >= 0), \"Attention weights should be non-negative\"\n",
    "    \n",
    "    # 2. Each row should sum to 1 (probability distribution)\n",
    "    row_sums = torch.sum(attention_weights, dim=-1)\n",
    "    assert torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-6), \"Attention weights should sum to 1 along last dimension\"\n",
    "    \n",
    "    print(\"✓ Basic functionality test passed!\")\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    print(f\"  Attention weights shape: {attention_weights.shape}\")\n",
    "    print(f\"  Attention weights sum: {row_sums[0, 0].item():.6f} (should be ~1.0)\")\n",
    "\n",
    "test_basic_functionality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a160cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
