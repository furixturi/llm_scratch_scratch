{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56db8b3",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b2c041",
   "metadata": {},
   "source": [
    "An implementation of scaled dot-product attention with PyTorch, optionally accepting a mask for causal attention. This is the core mechanism used in Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27dfbde",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4efc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=query.dtype))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa35a801",
   "metadata": {},
   "source": [
    "## Line-by-Line Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024898d0",
   "metadata": {},
   "source": [
    "### def \\_\\_init\\_\\_(self, dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a84cec",
   "metadata": {},
   "source": [
    "- First, instantiate the parent class `nn.Module` \n",
    "\n",
    "- `self.dropout = nn.Dropout(dropout)`: Initialize a Dropout layer to apply dropout to attention weights, where we randomly drop some attention scores. \n",
    "  - Why: Adding dropout helps prevent overfitting and better generalize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf49335",
   "metadata": {},
   "source": [
    "### def forward(self, query, key, value, mask=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc33954",
   "metadata": {},
   "source": [
    "This function implements the scaled dot-product attention formula below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee6e103",
   "metadata": {},
   "source": [
    "$\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left( \\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3737fad",
   "metadata": {},
   "source": [
    "The input tensors:\n",
    "- query, key, value are of the same shape: `(batch_size, seq_len, d_k)`\n",
    "  - `d_k` is the dimensionality of the query and key vectors. In single-head attention implementations, it equals `d_model`, aka the embedding size. In multi-head attention, it equals to `d_model / num_head`\n",
    "- mask shape is `(batch_size, seq_len, seq_len)`\n",
    "  - it's a binary mask, 1 = attend, 0 = ignore. We use it to ignore future tokens or padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90649878",
   "metadata": {},
   "source": [
    "Line-by-line:\n",
    "- Get the scale dimension: \n",
    "  - `d_k = query.size(-1)`  \n",
    "    - This is used for **scaling** the dot product later.\n",
    "- Compute attention scores: \n",
    "  - `scores = torch.matmul(query, key.transpose(-2, -1))/torch.sqrt(torch.tensor(d_k, dtype=query.dtype))` \n",
    "    - First, compute the dot products of `query @ keyᵀ` which indicate how much attention one position should pay to another\n",
    "    - Then, scale the dot products (divide by √d_k) to **scale down large values** and stablize softmax. This was recommended in the original Transformer paper\n",
    "- If mask was provided in the input, apply it\n",
    "  - `scores = scores.masked_fill(mask == 0, float('-inf'))` \n",
    "    - The mask is a binary mask with 1 for attend and 0 for ignore. Here we replace positions with 0 value with `-inf` in the mask, so that softmax scores them to zero, effectively paying zero attention to them.\n",
    "- Convert attention scores to attention weights (probability distribution):\n",
    "  - `attention_weights = F.softmax(scores, dim=-1)`  \n",
    "    - All weights then fall into the range of 0 and 1 and sum up to 1.\n",
    "- Apply dropout: \n",
    "  - `attention_weights = self.dropout(attention_weights)`  \n",
    "    - Apply dropout to attention weights regularizes the model by randomly zeroing some weights. This prevents model from depending too heavily on any one token during trianing, thus improves model robustness and generalization.\n",
    "- Calculate output, which is a weighted sum of the value vectors using the attention weights: \n",
    "  - `output = torch.matmul(attention_weights, value)` \n",
    "    - This combines information from all tokens (their values) according to how much attention should be paid to each of them\n",
    "    - output shape is also `(batch_size, seq_len, d_k)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f26f690",
   "metadata": {},
   "source": [
    "The output tensors:\n",
    "- output: context-aware information from all tokens. Shape: `(batch_size, seq_len, d_k)`\n",
    "  - This is used as input to the next layer. It is also used in backpropagation during training.\n",
    "- attention_weights: distribution of attention over all tokens. Shape: `(batch_size, seq_len, seq_len)` \n",
    "  - This shows attention distribution and can be used in visualization / interpretability analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83281f72",
   "metadata": {},
   "source": [
    "Note\n",
    "- Dropout is applied after Softmax, not before it. The reason is we want to randomly zero out token contributions (probabilities), not distort the probability distribution before calculating it from the similarity scores (softmax). This will result in the attention weights not sum up to 1 anymore but it is intentional during training. Re-normalize after dropout is also possible but not widely used. At inference, the dropout is disabled so that the token probability distribution sum up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976b966",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
