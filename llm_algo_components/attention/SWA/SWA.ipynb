{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f1fa66",
   "metadata": {},
   "source": [
    "# Sliding Window Attention (SWA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0530d6",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525113e",
   "metadata": {},
   "source": [
    "![SWA](Mistral%207B%20SWA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd1c2a9",
   "metadata": {},
   "source": [
    "Sliding Window Attention (SWA) is an efficient attention mechanism that restricts each token to attend to only a fixed-size window of neighboring tokens (previous tokens in the case of causal LMs), rather than all tokens. This reduces attention's time and memory complexity from $O(n^2)$ to $O(n \\cdot w)$, where $w$ is the window size ($w \\ll n$). Since the attention layers are stacked, tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by W tokens. Hence, after k attention layers, information can move forward by up to k Ã— W tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656955d1",
   "metadata": {},
   "source": [
    "In 2020, Allen Institute for AI published Longformer, which combined dilated sliding windows with optional global tokens to achieve linear scaling on long documents.\n",
    "\n",
    "In 2021, Google Research published BigBird, which used a sliding window + random + global sparsity schema and showed expressivity close to full attention\n",
    "\n",
    "Mistral 7B, released in 2023, adopted SWA. It uses a window size $W=4096$ and model depth $L=32$, $L \\times W = 131k$, effectively covered its 32K max context length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a6e89",
   "metadata": {},
   "source": [
    "The formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5753db6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{SWA}(Q, K, V)_i = \\text{softmax}\\left(\\frac{Q_i K_{[i-w+1:i]}^T}{\\sqrt{d_k}}\\right) V_{[i-w+1:i]}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e33d5",
   "metadata": {},
   "source": [
    "Where:\n",
    "- $w$ is the window size\n",
    "- $Q_i$ is the query at position i\n",
    "- $K_{[i-w+1:i]}$ are the keys within the window\n",
    "- $V_{[i-w+1:i]}$ are the values within the window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3213317",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d7a0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sliding_window_attention(Q, K, V, window_size):\n",
    "    \"\"\"\n",
    "    Causal Sliding Window Attention (no dilation).\n",
    "    Q, K, V: [B, T, H, D]\n",
    "    window_size: int W (tokens to the left, inclusive of current position)\n",
    "    Returns: [B, T, H, D]\n",
    "    \"\"\"\n",
    "    B, T, H, D = Q.shape\n",
    "    device = Q.device\n",
    "\n",
    "    # We will compute attention in banded blocks using an index trick:\n",
    "    # For each i, gather keys/values in [i-W+1, i] clipped to [0, T-1].\n",
    "\n",
    "    # Build indices: idx[i, k] = j in window k for position i\n",
    "    # k runs from 0..W-1 mapping to j = i - (W-1 - k)\n",
    "    W = min(window_size, T)\n",
    "    base = torch.arange(T, device=device).unsqueeze(1).expand(T, W)            # [T, W]\n",
    "    offsets = torch.arange(-(W-1), 1, device=device).unsqueeze(0).expand(T, W) # [T, W]\n",
    "    idx = (base + offsets).clamp(min=0)                                        # [T, W], j indices\n",
    "\n",
    "    # Gather K, V windows for each position\n",
    "    # K_win, V_win: [B, T, H, W, D]\n",
    "    K_win = K.gather(dim=1, index=idx.view(1, T, 1, W, 1).expand(B, T, H, W, D))\n",
    "    V_win = V.gather(dim=1, index=idx.view(1, T, 1, W, 1).expand(B, T, H, W, D))\n",
    "\n",
    "    # Compute scores: [B, T, H, W]\n",
    "    # scores_{i,k} = <Q_{i}, K_{idx[i,k]}>\n",
    "    Q_exp = Q.unsqueeze(3)                      # [B, T, H, 1, D]\n",
    "    scores = (Q_exp * K_win).sum(dim=-1)        # dot product -> [B, T, H, W]\n",
    "    scores = scores / (D ** 0.5)\n",
    "\n",
    "    # Mask out positions > i (causality) that slipped in via clamp at seq start\n",
    "    # Valid columns per row i are those where idx[i,k] <= i\n",
    "    arange_T = torch.arange(T, device=device).unsqueeze(1).expand(T, W)  # [T, W]\n",
    "    causal_mask = (idx <= arange_T)                                      # [T, W]\n",
    "    scores = scores.masked_fill(~causal_mask.view(1, T, 1, W), float('-inf'))\n",
    "\n",
    "    # Softmax over the W window entries and weighted sum\n",
    "    attn = F.softmax(scores, dim=-1)             # [B, T, H, W]\n",
    "    out = torch.einsum('bthw,bthwd->bthd', attn, V_win)  # [B, T, H, D]\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
