{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820ec24d",
   "metadata": {},
   "source": [
    "# Grouped-Query Attention (GQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa541c2",
   "metadata": {},
   "source": [
    "**Grouped-Query Attention (GQA)** is a variant of MHA that improves efficiency by **reducing the number of key and value heads** while keeping the number of query heads, i.e., letting multiple query heads share the same k/v heads. It was introduced by Google Research in 2023 paper \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\" [arxiv.org/abs/2305.13245](https://arxiv.org/abs/2305.13245)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a32d8",
   "metadata": {},
   "source": [
    "Standard MHA has one key-value pair per query and achieves the highest quality, but is also expensive in memory and compute. On the other end, multi-query attention (MQA) shares one key-value pair across all queries, trading off quality for efficiency. GQA strikes a balance by grouping query heads to share fewer key/value heads, enabling higher performance with fewer parameters in the attention model, which improves speed whie preserving quality. GQA enables lower KV cache memory usage and faster inference, better scalability in large models with long sequences. \n",
    "\n",
    "In practice, the number of query heads and the number of k/v heads are tunable hyperparameters. Number of query heads need to be divisible by number of kv heads. Both need to be divisible by the embedding dimension. For latency-sensitive use cases, raise the num_query_heads to num_kv_heads ratio. For more expressive training, lower the ratio so that fewer query heads share the same kv head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1de76",
   "metadata": {},
   "source": [
    "GQA is adopted by several major LLMs: PaLM, LLaMA 2 70B, LLaMA 3, DeepSeek-v2/v3, Qwen2/Qwen3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b3d415",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4326e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f7e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=query.dtype))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3bb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_query_heads, num_groups, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_query_heads == 0, \"embed_dim must be divisible by num_query_heads\"\n",
    "        assert num_query_heads % num_groups == 0, \"num_query_heads must be divisible by num_groups\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_query_heads = num_query_heads\n",
    "        self.query_head_dim = embed_dim // num_query_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.heads_per_group = num_query_heads // num_groups\n",
    "        self.kv_head_dim = self.query_head_dim * num_groups # Each group shares the same key/value head\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, self.kv_head_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, self.kv_head_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_length, embed_dim) - input tensor\n",
    "            mask: (batch_size, num_query_heads, seq_length, seq_length) - optional attention mask\n",
    "        Returns:\n",
    "            output: (batch_size, seq_length, embed_dim) - output tensor after attention\n",
    "            attn_weights: (batch_size, num_query_heads, seq_length, seq_length)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "\n",
    "        # Put input X through query learnable projection layer (weight matrix plus bias), split the embed_dim dimension to the number of query heads, then transpose the second and third dimensions for multi-head attention\n",
    "        # q is then of shape (batch_size, self.num_query_heads, seq_length, self.query_head_dim)\n",
    "        q = self.q_proj(x).view(batch_size, seq_length, self.num_query_heads, self.query_head_dim).transpose(1, 2)\n",
    "\n",
    "        # Put input X through the key and value learnable linear projection layer (weight matrices plus biases), split the embed_dim dimension to the number of key/value heads, then transpose the second and third dimensions for multi-head attention\n",
    "        # k and v are then of shape (batch_size, self.num_kv_heads, seq_length, self.kv_head_dim)\n",
    "        k = self.k_proj(x).view(batch_size, seq_length, self.num_kv_heads, self.kv_head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch_size, seq_length, self.num_kv_heads, self.kv_head_dim).transpose(1, 2)\n",
    "\n",
    "        # Repeat key/value heads to match the number of query heads\n",
    "        if self.num_query_heads != self.num_kv_heads:\n",
    "            k = k.repeat_interleave(self.num_query_heads // self.num_kv_heads, dim=1)\n",
    "            v = v.repeat_interleave(self.num_query_heads // self.num_kv_heads, dim=1)\n",
    "\n",
    "        # Apply attention\n",
    "        attn_output, attn_weights = self.attn(q, k, v, mask)\n",
    "\n",
    "        # Concatenate heads and project output\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad44c9",
   "metadata": {},
   "source": [
    "## Line-by-line Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ef46f",
   "metadata": {},
   "source": [
    "### def \\_\\_init\\_\\_(self, embed_dim, num_query_heads, num_kv_heads, dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e845422e",
   "metadata": {},
   "source": [
    "- First, make sure that embedding dimension is divisible by both the number of query heads and the number of k/v heads\n",
    "  - ```\n",
    "    assert embed_dim % num_query_heads == 0, \"embed_dim must be divisible by num_query_heads\"\n",
    "    assert embed_dim % num_kv_heads == 0, \"embed_dim must be divisible by num_kv_heads\"\n",
    "    ```\n",
    "- store the embedding dimension, number of query heads and number of k/v heads to instance attributes\n",
    "  - ```\n",
    "    self.embed_dim = embed_dim\n",
    "    self.num_query_heads = num_query_heads\n",
    "    self.num_kv_heads = num_kv_heads\n",
    "    ```\n",
    "- calculate query head dimension and k/v head dimension\n",
    "  - ```\n",
    "    self.q_head_dim = embed_dim // num_query_heads\n",
    "    self.kv_head_dim = embed_dim // num_kv_heads\n",
    "    ```\n",
    "- create the learnable weight matrices\n",
    "  - ```\n",
    "    self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f6c68",
   "metadata": {},
   "source": [
    "### def forward(self, x, mask=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563f19c",
   "metadata": {},
   "source": [
    "The input tensors are\n",
    "- x: (batch_size, seq_len, embed_dim)\n",
    "- mask: optional attention mask for causal or padding, should be in shape (batch_size, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5905de34",
   "metadata": {},
   "source": [
    "Line-by-line implementation:\n",
    "- Get the batch size and sequence length from the input X's shape\n",
    "  - `batch_size, seq_length, _ = x.shape`\n",
    "- Compute the Q, K, V tensors and prepare them for GQA\n",
    "  - ```\n",
    "    q = self.q_proj(x).view(batch_size, seq_length, self.num_query_heads, self.q_head_dim).transpose(1, 2)\n",
    "    k = self.k_proj(x).view(batch_size, seq_length, self.num_kv_heads, self.kv_head_dim).transpose(1, 2)\n",
    "    v = self.v_proj(x).view(batch_size, seq_length, self.num_kv_heads, self.kv_head_dim).transpose(1, 2)\n",
    "    ```\n",
    "    - First put the input through the corresponding learnable linear projection layer\n",
    "    - Then split the last dimension (embed_dim) into the corresponding number of heads dimension and head dimension \n",
    "    - Finally, transpose the seq_length and num_xx_heads dimensions for grouped multi-head attention\n",
    "- Share kv heads across query heads (If there are fewer kv heads than query heads, duplicate kv heads to match the number of query heads, as the dot product is happening on this dimension)\n",
    "  - ```\n",
    "    if self.num_query_heads != self.num_kv_heads:\n",
    "        k = k.repeat_interleave(self.num_query_heads // self.num_kv_heads, dim=1)\n",
    "        v = v.repeat_interleave(self.num_query_heads // self.num_kv_heads, dim=1)\n",
    "    ```\n",
    "      - after this, k, v tensor shape becomes `(batch_size, num_query_heads, seq_len, kv_head_dim)`\n",
    "      - q tensor shape is `(batch_size, num_query_heads, seq_len, q_head_dim)`\n",
    "- Compute scaled dot-product attention of each head\n",
    "  -  `attn_output, attn_weights = self.attn(q, k, v, mask)`\n",
    "     -  attn_output shape: `(batch_size, num_query_heads, seq_len, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df85c43c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
