{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "434b0eb6",
   "metadata": {},
   "source": [
    "# Rotary Position Embedding (RoPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c92de1",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1cc5ac",
   "metadata": {},
   "source": [
    "Self-attention is permutation-invariatnt. Each query token attends to all its context tokens simultaneously, meaning, unlike RNN/CNNs, Transformers have no built-in notion of word order. \n",
    "\n",
    "Like previous positional encoding techniques, **Rotary Position Embedding (RoPE)** is developed to enable the model to learn about token position information. It was introduced by Jianlin Su et al. in their 2021 paper *[RoFormer: Enhanced Transformer with Rotatry Position Embedding](https://arxiv.org/abs/2104.09864)* and was widely adopted in famous OSS LLMs (sometimes with modifications like YaRN, NTK-aware scaling or Dynamic NTK) such as GPT-NeoX (EleutherAI), LLaMA/LLaMA-2/LLaMA-3, and Qwen series. \n",
    "\n",
    "RoPE is the de-facto technique for LLM positional encoding now. One key advantage of RoPE, compared to Sinusoidal Positional Encodeing, is that RoPE preserves the norm and geometry of Q/K while injecting position. Additive sinusoidal can distort the magnitude and direction of the embeding, thus interfere with attention scores. \n",
    "\n",
    "RoPE is especially beneficial when scaling to billions of parameters:\n",
    "- It tends to enable more stable training\n",
    "- It makes sequence length extrapolation (context extension) straightforward\n",
    "- It produces cleaner long-range generalization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8f924",
   "metadata": {},
   "source": [
    "## The Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb1577c",
   "metadata": {},
   "source": [
    "Similar to Sinusoidal Positional Encoding, RoPE maps token positions to multiple sine-cosine frequency signals, each frequency $\\phi$ is tied to a specific consecutive pair of model or head dimensions ($2m$ for even channels, $2m+1$ for odd channels). With position $p$ and dimension pair index $m$, the frequency $\\phi_m(p)$ is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b158d1e1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\phi_m(p) = p \\cdot B^{-\\frac{2m}{d}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e15b7",
   "metadata": {},
   "source": [
    "- $\\phi$: sine/cosine frequency\n",
    "- $p$: token position\n",
    "- $B$: the base constant, 10000 by default\n",
    "- $m$: dimension pair index ($2m$ for even and $2m+1$ for odd)\n",
    "- $d$: dimension size of the model or one attention head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faad212",
   "metadata": {},
   "source": [
    "In Sinusoidal Position Encoding, this frequency $\\phi$ is used as the angle to calculate the sine-cosine values which then get **added** to embeddings. In RoPE, this is the angle used in the **rotation matrix** that multiplies each Q/K pair $(q_{2m}, q_{2m+1})$ or $(k_{2m}, k_{2m+1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569424d3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "q'_{2m}\\\\ q'_{2m+1}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\cos\\phi_m(p) & -\\sin\\phi_m(p) \\\\\n",
    "\\sin\\phi_m(p) & \\cos\\phi_m(p)\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "q_{2m}\\\\ q_{2m+1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "k'_{2m}\\\\ k'_{2m+1}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\cos\\phi_m(p) & -\\sin\\phi_m(p) \\\\\n",
    "\\sin\\phi_m(p) & \\cos\\phi_m(p)\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "k_{2m}\\\\ k_{2m+1}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fb334",
   "metadata": {},
   "source": [
    "### Math recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce175d0b",
   "metadata": {},
   "source": [
    "A recap of rotation matrix and trig identities formula (in case that high-school math got rusty)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3ce07",
   "metadata": {},
   "source": [
    "#### Angle-sum and -difference Trigonometric Identities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3af3e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sin(\\alpha + \\beta) = \\sin\\alpha\\cos\\beta + \\cos\\alpha\\sin\\beta \\\\\n",
    "\\sin(\\alpha - \\beta) = \\sin\\alpha\\cos\\beta - \\cos\\alpha\\sin\\beta \\\\\n",
    "\\cos(\\alpha + \\beta) = \\cos\\alpha\\cos\\beta - \\sin\\alpha\\sin\\beta \\\\\n",
    "\\cos(\\alpha - \\beta) = \\cos\\alpha\\cos\\beta + \\sin\\alpha\\sin\\beta \\\\\n",
    "\\tan(\\alpha + \\beta) = \\frac{\\tan\\alpha + \\tan\\beta}{1 - \\tan\\alpha\\tan\\beta} \\\\\n",
    "\\tan(\\alpha - \\beta) = \\frac{\\tan\\alpha - \\tan\\beta}{1 + \\tan\\alpha\\tan\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cc559",
   "metadata": {},
   "source": [
    "#### Rotation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc112d59",
   "metadata": {},
   "source": [
    "Then, let's use polar coordinates so that the rotation becomes obvious. The following is the **rotation matrix**. To rotate a vector counter-clockwise by the angle $\\phi$, simply compute the dot product of this rotate matrix and the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24350d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\phi) & -\\sin(\\phi)\\\\\n",
    "\\sin(\\phi) & \\cos(\\phi)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f97bfe",
   "metadata": {},
   "source": [
    "A point in Cartesian coordinates A(x, y) can also be represented as a vector in polar coordinates $A(r\\cos\\alpha, r\\sin\\alpha)$, where $r$ is the scale and $\\alpha$ is the angle counter-clockwise from 0. We can apply this to the rotation matrix dot-product:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2378baea",
   "metadata": {},
   "source": [
    "$$ \n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "x\\\\ y\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "r\\cos\\alpha\\\\ r\\sin\\alpha\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$ \n",
    "A' = \n",
    "\\begin{bmatrix}\n",
    "x'\\\\ y'\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\cos\\phi & -\\sin\\phi\\\\\n",
    "\\sin\\phi & \\cos\\phi\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "x\\\\ y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\cos\\phi & -\\sin\\phi\\\\\n",
    "\\sin\\phi & \\cos\\phi\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "r\\cos\\alpha\\\\ r\\sin\\alpha\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "r(\\cos\\phi\\cos\\alpha-\\sin\\phi\\sin\\alpha)\\\\\n",
    "r(\\sin\\phi\\cos\\alpha+\\cos\\phi\\sin\\alpha)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d8edd7",
   "metadata": {},
   "source": [
    "Applying the trig identities formula of $cos(\\alpha+\\beta)$ and $sin(\\alpha+\\beta)$, we know that the above equals to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2342acc2",
   "metadata": {},
   "source": [
    "$$\n",
    "A' = \n",
    "\\begin{bmatrix}\n",
    "r\\cos(\\alpha + \\phi) \\\\\n",
    "r\\sin(\\alpha + \\phi)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f0370",
   "metadata": {},
   "source": [
    "Which is obviously $A(r\\cos\\alpha, r\\sin\\alpha)$ rotated counter-clockwise by angle $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b5e0a1",
   "metadata": {},
   "source": [
    "#### Complex number representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d932f",
   "metadata": {},
   "source": [
    "The original RoFormer paper defined RoPE in the **complex number** domain (\"We view each two consecutive channels of a query/key vector as the real and imaginary parts of a complex number and multiply it by a unit complex number $e^{i\\theta}$\".) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10112fde",
   "metadata": {},
   "source": [
    "In the complex plane, multiplying a coplex number $z = x + iy$ by $e^{i\\theta}$ rotates it counter-clockwise by $\\theta$ radians without changing its length:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f1137",
   "metadata": {},
   "source": [
    "$$\n",
    "z' = z \\cdot e^{i\\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866a5fac",
   "metadata": {},
   "source": [
    "According to **Euler's identity formula**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6651f",
   "metadata": {},
   "source": [
    "$$\n",
    "e^{i\\theta} = \\cos\\theta + i\\sin\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb39416",
   "metadata": {},
   "source": [
    "So Multiplying $z$ and $e^{i\\theta}$ gives us:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae68804",
   "metadata": {},
   "source": [
    "$$\n",
    "z' = z \\cdot e^{i\\theta} \\\\\n",
    "= (x + iy)(\\cos\\theta + i\\sin\\theta) \\\\\n",
    "= x\\cos\\theta + ix\\sin\\theta + iy\\cos\\theta + i^2y\\sin\\theta \\\\\n",
    "= x\\cos\\theta + ix\\sin\\theta + iy\\cos\\theta + (-1)y\\sin\\theta \\\\\n",
    "= x\\cos\\theta - y\\sin\\theta + i(x\\sin\\theta + y\\cos\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76d813",
   "metadata": {},
   "source": [
    "We can interpret this as a matrix, so that the real part is the new $x'$, the imaginary part is the new $y'$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0c12e",
   "metadata": {},
   "source": [
    "$$\n",
    "z' = \n",
    "\\begin{bmatrix}\n",
    "x' \\\\ y'\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\cos\\theta & -\\sin\\theta \\\\\n",
    "\\sin\\theta & \\cos\\theta\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "x \\\\ y\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f829ca",
   "metadata": {},
   "source": [
    "This tells us that multiplying $e^{i\\theta}$ with vector $z=[x, y]^T$ in the complex plane is exactly the same as left-multiplying the 2D rotation matrix $R_{\\theta}$ with the 2D vector $[x, y]^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c7c3b",
   "metadata": {},
   "source": [
    "The math of the original RoFormer paper is: \n",
    "$$ z' = z \\cdot e^{ip\\theta_m} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb85af",
   "metadata": {},
   "source": [
    "Where $p$ is sequence position and $m$ indexes 2D pairs of model dimension channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef2d0b",
   "metadata": {},
   "source": [
    "### Real World Implementation Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b6af6",
   "metadata": {},
   "source": [
    "Despite the original paper using complex notation, most OSS LLM implementation (LLaMA, Qwen, etc.) implement RoPE in real space using the explicit 2x2 rotation formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b55616",
   "metadata": {},
   "source": [
    "$$\n",
    "q'_{2m} = q_{2m}\\cos\\phi_m - q_{2m+1}\\sin\\phi_m \\\\\n",
    "q'_{2m+1} = q_{2m}\\sin\\phi_m + q_{2m+1}\\cos\\phi_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab98fc",
   "metadata": {},
   "source": [
    "The reason could be\n",
    "- Many frameworks (e.g. PyTorch) didn't have complex number dtype support\n",
    "- Explicit real number operations can be faster on GPU as there's no need for type conversions\n",
    "- It is easier to integrate into existing fused attention kernels like FlashAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ba678",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14986a7a",
   "metadata": {},
   "source": [
    "Here is a minimalistic and straightforward implementation of RoPE following the formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d5e442",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_m = B^{\\frac{-2m}{d_{\\text{head}}}} \\\\\n",
    "\\phi_m(p) = p\\cdot\\theta_m \\\\\n",
    "\\begin{bmatrix}\n",
    "x'_{2m} \\\\\n",
    "x'_{2m+1}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\cos\\phi & -\\sin\\phi \\\\\n",
    "\\sin\\phi & \\cos\\phi\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "x_{2m} \\\\\n",
    "x_{2m+1}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ab415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def build_rope_cache(max_seq_len: int, head_dim: int, base: float = 10000.0, \n",
    "                     pos_scale: float = 1.0, # set this >1.0 to extend the context (simple NTK-style)\n",
    "                     device = None,\n",
    "                     dtype = torch.float32):\n",
    "    \"\"\"\n",
    "    Implements theta_m = base^(-2m/d_head), phi_m(p) = p/pos_scale * theta_m\n",
    "    Build RoPE cache for cos(phi_m) and sin(phi_m) values\n",
    "    \n",
    "    Returns:\n",
    "        cos, sin: [max_seq_len, head_dim//2], lookup tables of cosine and sine values ready to slice/gather and broadcast\n",
    "    \"\"\"\n",
    "    assert head_dim % 2 == 0, \"head_dim must be even for RoPE\"\n",
    "\n",
    "    # Create dimension indices m=0, 1, ..., head_dim//2-1 for each frequency\n",
    "    m = torch.arange((head_dim // 2), device=device, dtype=dtype) \n",
    "    \n",
    "    # Calculate all dimension frequencies theta_m = base^(-2m/d_head)\n",
    "    theta_m = base ** (-2.0 * m / head_dim) # [head_dim//2]\n",
    "\n",
    "    # Create sequence position indices with scaling\n",
    "    positions = torch.arange(max_seq_len, device=device, dtype=dtype) / pos_scale # [max_seq_len]\n",
    "\n",
    "    # Calculate angles for each position-dimension combination pair phi_m(pos) = pos * theta_m\n",
    "    angles = positions[:, None] * theta_m[None, :] # [max_seq_len, head_dim//2]\n",
    "\n",
    "    # Calculate cos, sin values and fill the lookup cache table\n",
    "    cos = torch.cos(angles) # [max_seq_len, head_dim//2]\n",
    "    sin = torch.sin(angles) # [max_seq_len, head_dim//2]\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    \"\"\"\n",
    "    Apply RoPE rotation to input tensor using precomputed cos/sin values\n",
    "\n",
    "    Inputs:\n",
    "        x: input tensor - [batch_size, seq_len, num_heads, head_dim]\n",
    "        cos: precomputed cosine values - [max_seq_len, head_dim]\n",
    "        sin: precoputed sine values - [max_seq_len, head_dim]\n",
    "\n",
    "    Returns:\n",
    "        rotated tensor - [batch_size, seq_len, num_heads, head_dim]\n",
    "    \"\"\"\n",
    "    # Slice cos/sin to match input seq_len\n",
    "    seq_len = x.size(1)\n",
    "    cos = cos[:seq_len]\n",
    "    sin = sin[:seq_len]\n",
    "\n",
    "    # Split input to odd/even pairs\n",
    "    x_even = x[..., 0::2]   # [batch_size, seq_len, num_heads, head_dim//2]\n",
    "    x_odd = x[..., 1::2]    # [batch_size, seq_len, num_heads, head_dim//2]    \n",
    "\n",
    "    # Apply rotation matrix multiplication:\n",
    "    # [x_rotated_even; x_rotated_odd] = [cos -sin; sin cos] * [x_even; x_odd]\n",
    "    x_rotated_even = cos * x_even - sin * x_odd\n",
    "    x_rotated_odd = sin * x_even + cos * x_odd\n",
    "\n",
    "    # Interleave back to original input size\n",
    "    x_rotated = torch.empty_like(x)\n",
    "    x_rotated[..., 0::2] = x_rotated_even\n",
    "    x_rotated[..., 1::2] = x_rotated_odd\n",
    "\n",
    "    return x_rotated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20145737",
   "metadata": {},
   "source": [
    "A sample MHA module using this implementation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc4a73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class RopeMHA(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_length=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # initialize Q, K, V, output projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Build RoPE cache\n",
    "        self.register_buffer(\"rope_cos\", None)\n",
    "        self.register_buffer(\"rope_sin\", None)\n",
    "        self._build_rope_cache(max_length, self.head_dim)\n",
    "    \n",
    "    def _build_rope_cache(self, max_length: int, head_dim: int, \n",
    "                          base: float = 10000.0,\n",
    "                          pos_scale: float = 1.0,\n",
    "                          device: torch.device | None = None,\n",
    "                          dtype = torch.float32):\n",
    "        assert head_dim % 2 == 0, \"head_dim must be even\"\n",
    "        # Calculate dimension half indices vector\n",
    "        m = torch.arange(head_dim // 2, device=device, dtype=dtype)\n",
    "        theta_m = base ** (- 2 * m / head_dim)\n",
    "        # Build positions vector\n",
    "        pos = torch.arange(max_length, device=device, dtype=dtype) / pos_scale\n",
    "        # Compute angles of all dim-pos combinations\n",
    "        angles = pos[:, None] * theta_m[None, :]\n",
    "        # Compute cos/sin values and cache them\n",
    "        cos = torch.cos(angles)\n",
    "        sin = torch.sin(angles)\n",
    "\n",
    "        self.register_buffer(\"rope_cos\", cos)\n",
    "        self.register_buffer(\"rope_sin\", sin)\n",
    "    \n",
    "    def _apply_rope(self, x):\n",
    "        # Get cos and sin from cache\n",
    "        seq_len = x.size(1)\n",
    "        cos, sin = self.rope_cos[:seq_len], self.rope_sin[:seq_len]\n",
    "        # Split x to even and odd pairs\n",
    "        x_even = x[..., 0::2]\n",
    "        x_odd = x[..., 1::2]\n",
    "        # Rotate\n",
    "        x_rotated_even = cos * x_even - sin * x_odd\n",
    "        x_rotated_odd = sin * x_even + cos * x_odd\n",
    "        # Build the full rotated tensor\n",
    "        x_rotated = torch.empty_like(x)\n",
    "        x_rotated[..., 0::2] = x_rotated_even\n",
    "        x_rotated[..., 1::2] = x_rotated_odd\n",
    "\n",
    "        return x_rotated\n",
    "        \n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # Project to get Q, K, V, and reshape for multi-head\n",
    "        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Apply RoPE to Q, K\n",
    "        Q = self._apply_rope(Q)\n",
    "        K = self._apply_rope(K)\n",
    "\n",
    "        # Transpose for parallel multi-head attention\n",
    "        Q = Q.transpose(1,2)\n",
    "        K = K.transpose(1,2)\n",
    "        V = V.transpose(1,2)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(attn_scores)\n",
    "        attn_values = torch.matmul(attn_weights, V)\n",
    "\n",
    "        output = self.out_proj(attn_values.transpose(1,2).reshape(batch_size, seq_len, d_model))\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
