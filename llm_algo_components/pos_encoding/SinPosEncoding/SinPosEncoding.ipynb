{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae4d2c0",
   "metadata": {},
   "source": [
    "# Sinusoidal (Fixed) Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033bea95",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2297f9e5",
   "metadata": {},
   "source": [
    "Self-attention is permutation-invariatnt. Each query token attends to all its context tokens simultaneously, meaning, unlike RNN/CNNs, Transformers have no built-in notion of word order. \n",
    "\n",
    "To inject token order, Sinusoidal Position Encoding was introduced in the original Transformer (\"Attention is All You Need\", Vaswani et al. 2017). The core idea is to encode each position $p \\in [0, L)$ as a deterministic vector of size d_model $PE(p) \\in \\mathbb{R}^d$ that represents positional information with sinusoidal (sines and cosines) at geometrically spaced frequencies (higher dimension gets higher frequency and vice versa). In this way\n",
    "   -  Any position can be computed on the fly, it is determnistic, works out-of-the-box and requires no additional parameters.\n",
    "   -  Adding this to embeddings lets attention also \"sense\" the order and distance (*relative offsets*) between tokens. The relative position signal emerges linearly, since sin(p+delta), cos(p+delta) are linear combos of sin(p), cos(p), a linear layer can recover the offset delta.\n",
    "   -  Enabling extrapolation to longer sequences than seen in training, although quality can degrade as length gets too long.\n",
    "\n",
    "With Sinusoidal Positional Encoding, the model can learn to attend to relative or absolute positions via arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e572de4",
   "metadata": {},
   "source": [
    "The formula from the original Transformer paper:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = sin(\\frac{pos}{10000^{\\frac{2i}{d}}}) \n",
    "$$\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = cos(\\frac{pos}{10000^{\\frac{2i}{d}}})\n",
    "$$\n",
    "\n",
    "Where\n",
    "- $d$: model dimension\n",
    "- $i$: pair index to map to each feature dimension of the embedding vector. $i = 0, 1, ..., \\frac{d_{model}}{2}-1$. The pair index is used to get odd and even indices and the actual dimension index is $2i$ for even indices and $2i+1$ for odd indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d140a",
   "metadata": {},
   "source": [
    "Used in\n",
    "- original Transformer (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb15b4c",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17eb474",
   "metadata": {},
   "source": [
    "### Division Term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c96428b",
   "metadata": {},
   "source": [
    "In the formula the division term is $10000^{\\frac{2i}{d}}$. In code, for numerical stability and efficiency, we implement the original power form with its mathematically equivalent exponential form: \n",
    "\n",
    "$$\n",
    "\\frac{1}{10000^{\\frac{2i}{d}}} = 10000^{-\\frac{2i}{d}} = e^{ln(10000^{(-\\frac{2i}{d})})}\n",
    "$$\n",
    "$$\n",
    "= e^{-\\frac{2i}{d}ln10000}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0565dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "\n",
    "d_model = 20000 # example model dimension\n",
    "i = torch.arange(0, d_model, 2) # 2i in the formula, i = 0, 2, 4, ...\n",
    "div_term = torch.exp(-i / d_model * math.log(10000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17d457",
   "metadata": {},
   "source": [
    "### Sinusoidal Positional Encoding  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca025c",
   "metadata": {},
   "source": [
    "A minimalistic implementation of the formula:\n",
    "$$\n",
    "PE_{(pos, 2i)} = sin(\\frac{pos}{10000^{\\frac{2i}{d}}}) \n",
    "$$\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = cos(\\frac{pos}{10000^{\\frac{2i}{d}}})\n",
    "$$ \n",
    "\n",
    "with the exponential form of the division term as implemented above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "def sinusoidal_position_encoding(seq_len: int, d_model: int):\n",
    "    \"\"\"\n",
    "    Input: seq_len, d_model\n",
    "    Output: a positional encoding tensor of shape (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(seq_len, d_model)  # PE matrix: (seq_len, d_model)\n",
    "    pos = torch.arange(seq_len).unsqueeze(1)    # positions 0, 1, 2,..., seq_len-1 -> (seq_len, 1)\n",
    "    \n",
    "    # calculate div_term\n",
    "    i = torch.arange(0, d_model, 2) # 2i in the fromula, = 0, 2, 4, ..., d_model-1\n",
    "    div_term = torch.exp(-i / d_model * math.log(10000.0))\n",
    "\n",
    "    # fill the PE matrix odd and even channels\n",
    "    pe[:, 0::2] = torch.sin(pos * div_term) # 0, 2, 4, ..., d_model-1\n",
    "    pe[:, 1::2] = torch.cos(pos * div_term) # 1, 3, 5, ..., d_model-1\n",
    "\n",
    "    return pe\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31dede0",
   "metadata": {},
   "source": [
    "To use this to process the input in an autoregressive Transformer, simply add it to the input embedding. E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aeda51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# example values\n",
    "vocab_size = 32000\n",
    "d_model = 256\n",
    "seq_len = 128\n",
    "batch_size = 8\n",
    "\n",
    "# Dummy input tokenized IDs - (batch_size, seq_len)\n",
    "input_ids = torch.randint(low=0, high=vocab_size, size=(batch_size, seq_len))\n",
    "# Embedding layer - (vocab_size, d_model)\n",
    "token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "# Embedded input x - (batch_size, seq_len, d_model)\n",
    "x = token_embedding(input_ids)\n",
    "pe = sinusoidal_position_encoding(seq_len, d_model) # (seq_len, d_model)\n",
    "x = x + pe.unsqueeze(0) # broadcast pe on batch_size dimension\n",
    "\n",
    "# x is ready for attention layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
